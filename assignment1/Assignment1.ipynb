{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from multiprocessing import Pool, Lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'ml-1m'\n",
    "\n",
    "movies_filename = 'movies.dat'\n",
    "users_filename = 'users.dat'\n",
    "ratings_filename = 'ratings.dat'\n",
    "\n",
    "movie_columns = ['MovieID', 'Title', 'Genres']\n",
    "user_columns = ['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code']\n",
    "rating_columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "\n",
    "\n",
    "def make_dataframe(data_dir, filename, columns):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a data file\n",
    "    \n",
    "    data_dir: string, directory of data files\n",
    "    filename: string, data filename\n",
    "    columns: list, names for the columns of the dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    data_file = os.path.join(data_dir, filename)\n",
    "    return pd.read_csv(data_file, delimiter='::', names=columns, encoding='latin-1', engine='python')\n",
    "\n",
    "\n",
    "# Make the data frames for each data file\n",
    "movies = make_dataframe(data_dir, movies_filename, movie_columns)\n",
    "users = make_dataframe(data_dir, users_filename, user_columns)\n",
    "ratings = make_dataframe(data_dir, ratings_filename, rating_columns)\n",
    "data = (users, movies, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(errors):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error for an array of errors\n",
    "    \n",
    "    errors: array, array containing errors\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.square(errors)))\n",
    "\n",
    "\n",
    "def mae(errors):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute error for an array of errors\n",
    "    \n",
    "    errors: array, array containing errors\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(errors))\n",
    "\n",
    "\n",
    "def crop_ratings(arr):\n",
    "    \"\"\"\n",
    "    Ratings can only have values between 1.0 and 5.0. \n",
    "    This function sets all ratings > 5 to 5 and < 1 to 1.\n",
    "    \n",
    "    arr: array, array containing ratings\n",
    "    \"\"\"\n",
    "    new_arr = np.where(arr > 5.0, 5.0, arr)\n",
    "    new_arr = np.where(new_arr < 1.0, 1.0, new_arr)\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def rating_errors(data_set, model):\n",
    "    \"\"\"\n",
    "    Calculates the rmse and mae of the errors between ratings from and data set\n",
    "    and ratings predicted by a model.\n",
    "    \n",
    "    test_set: Dataframe, dataframe with the test data\n",
    "    model: function, function that can act on the rows of a dataframe\n",
    "    \"\"\"\n",
    "    errors = data_set['Rating'] - data_set.apply(model, axis=1)\n",
    "    return rmse(errors), mae(errors)\n",
    "\n",
    "\n",
    "def ids_to_indices(movie_ids):\n",
    "    \"\"\"\n",
    "    We want for each movie ID the corresponding index of an array. For the users this \n",
    "    is straightforward, since the the user ID's are integers from 1 to the number of \n",
    "    users and we can just subtract 1 to get all the indices. For some reason, some integers \n",
    "    are skipped in the movie ID's, so we cannot use them directly as indices of an array. \n",
    "    \n",
    "    For example: there is no movie with ID '91', so we want the movie with ID '92' \n",
    "    to correspond to an index of 90 (note that the first movie ID is '1' which \n",
    "    corresponds to index 0)\n",
    "    \n",
    "    Therefore we make an indices array with holes, which has parts like \n",
    "    [0, 1, ..., 218, 0, 219, 220, 221, ...]. The IDs '91' and '221' are missing. So a movie \n",
    "    with ID '222' will then take the 221st element of this array, which will take the value\n",
    "    219 for the index, because 2 zeros are inserted for the missing IDs.\n",
    "    \n",
    "    movie_ids: array, array containing the ID's of all movies\n",
    "    \"\"\"\n",
    "    ids_to_indices_arr = np.array([], dtype=np.int32)\n",
    "    skipped_integers = 0\n",
    "    for i in np.arange(len(movie_ids)):\n",
    "        if movie_ids[i] == i + 1 + skipped_integers:\n",
    "            ids_to_indices_arr = np.append(ids_to_indices_arr, i)\n",
    "        else:\n",
    "            while movie_ids[i] != i + 1 + skipped_integers:\n",
    "                ids_to_indices_arr = np.append(ids_to_indices_arr, 0)\n",
    "                skipped_integers += 1\n",
    "            ids_to_indices_arr = np.append(ids_to_indices_arr, i)\n",
    "    return ids_to_indices_arr\n",
    "\n",
    "\n",
    "def make_movie_indices(data_set, movie_ids):\n",
    "    \"\"\"\n",
    "    For a certain data set with movie ID's, return the corresponding indices of an array.\n",
    "    \n",
    "    data_set: Dataframe, dataframe containing ratings\n",
    "    movie_ids: array, array containing the ID's of all movies\n",
    "    \"\"\"\n",
    "    data_set_movie_ids = data_set['MovieID'].values - 1\n",
    "    ids_to_indices_arr = ids_to_indices(movie_ids)\n",
    "    movie_indices = ids_to_indices_arr[data_set_movie_ids]\n",
    "    return movie_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive models 1, 2 and 3\n",
    "\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_naive_model(mean_rating, train_set, model_type, model_specific_params):\n",
    "    \"\"\"\n",
    "    Builds the naive model of a given type.\n",
    "    \n",
    "    mean_ratings: float, the mean of all ratings (in the train set)\n",
    "    train_set: Dataframe, dataframe containing the training data\n",
    "    model_type: string, type of the model (use '1', '2' or '3')\n",
    "    model_specific_parmas: dict, dictionary containing objects specific to the model type\n",
    "    \"\"\"\n",
    "    if model_type == '1':\n",
    "        def model(row):\n",
    "            return mean_rating\n",
    "        return model\n",
    "    elif model_type == '2':\n",
    "        mean_rating_per_user = np.array([])\n",
    "        \n",
    "        # Calculate the mean rating for every user in the training set\n",
    "        for user_id in model_specific_params['user_ids']:\n",
    "            user_subset = train_set[train_set['UserID'].values == user_id]\n",
    "            mean_rating_per_user = np.append(mean_rating_per_user, np.mean(user_subset['Rating']))\n",
    "        \n",
    "        # Replace the user mean rating by the global mean rating if it is not available\n",
    "        mean_rating_per_user = np.where(np.isnan(mean_rating_per_user), mean_rating, mean_rating_per_user)\n",
    "\n",
    "        def model(row):\n",
    "            user_id = row['UserID'] - 1\n",
    "            return mean_rating_per_user[user_id]\n",
    "        return model\n",
    "    elif model_type == '3':\n",
    "        mean_rating_per_movie = np.array([])\n",
    "        \n",
    "        # Calculate the mean rating for every movie in the training set\n",
    "        for movie_id in model_specific_params['movie_ids']:\n",
    "            movie_subset = train_set[train_set['MovieID'].values == movie_id]\n",
    "            mean_rating_per_movie = np.append(mean_rating_per_movie, np.mean(movie_subset['Rating']))\n",
    "        \n",
    "        # Replace the movie mean rating by the global mean rating if it is not available\n",
    "        mean_rating_per_movie = np.where(np.isnan(mean_rating_per_movie), mean_rating, mean_rating_per_movie)\n",
    "\n",
    "        def model(row):\n",
    "            movie_id = row['MovieID'] - 1\n",
    "            return mean_rating_per_movie[model_specific_params['ids_to_indices_arr'][movie_id]]\n",
    "        return model\n",
    "    else:\n",
    "        return ValueError(f'Model type {model_type} is unknown.')\n",
    "\n",
    "    \n",
    "def naive_model_fold_error(ratings, train_indices, test_indices, model_type, model_specific_params):\n",
    "    \"\"\"\n",
    "    For a given fold, build the naive model and calculate its error on the test set.\n",
    "    \n",
    "    ratings: Dataframe, dataframe containing all ratings\n",
    "    train_indices: array, indices of the training set\n",
    "    test_indices array, indices of the test set\n",
    "    model_type: string, type of the model (use '1', '2' or '3')\n",
    "    model_specific_parmas: dict, dictionary containing objects specific to the model type\n",
    "    \"\"\"\n",
    "    train_set = ratings.iloc[train_indices]\n",
    "    test_set = ratings.iloc[test_indices]\n",
    "\n",
    "    mean_rating = train_set['Rating'].mean()\n",
    "\n",
    "    model = build_naive_model(mean_rating, train_set, model_type, model_specific_params)\n",
    "    \n",
    "    train_rmse, train_mae = rating_errors(train_set, model)\n",
    "    test_rmse, test_mae = rating_errors(test_set, model)\n",
    "    \n",
    "    return train_rmse, test_rmse, train_mae, test_mae\n",
    "\n",
    "\n",
    "def test_naive_model(data, model_type, n_folds=5):\n",
    "    \"\"\"\n",
    "    Calculates the error for a given model. Bias in training and test set selection is reduced\n",
    "    by using cross-validation. To speed up the process, we use multiprocessing to divide the \n",
    "    folds over different cores.\n",
    "    \n",
    "    data: tuple, contains the user, movie and rating dataframe\n",
    "    model_type: string, type of the model (use '1', '2' or '3')\n",
    "    n_folds: int, the number of folds to use for cross-validation\n",
    "    \"\"\"\n",
    "    users, movies, ratings = data\n",
    "    \n",
    "    cv = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n",
    "    \n",
    "    print(f'Testing naive model {model_type}...')\n",
    "    \n",
    "    if model_type == '1':\n",
    "        model_specific_params = {}\n",
    "    elif model_type == '2':\n",
    "        model_specific_params = {'user_ids': users['UserID']}\n",
    "    elif model_type == '3':\n",
    "        model_specific_params = {'movie_ids': movies['MovieID']}\n",
    "        model_specific_params.update({'ids_to_indices_arr': \n",
    "                                      ids_to_indices(model_specific_params['movie_ids'])})\n",
    "    else:\n",
    "        return ValueError(f'Model type {model_type} is unknown.')\n",
    "    \n",
    "    # Simulatneously calculate the errors for each fold\n",
    "    params = [(ratings,\n",
    "               train_indices, \n",
    "               test_indices, \n",
    "               model_type,\n",
    "               model_specific_params) for train_indices, test_indices in cv.split(ratings)]\n",
    "    pool = Pool(n_folds)\n",
    "    fold_errors = pool.starmap(naive_model_fold_error, params)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # fold_errors has dimensions [n_folds, n_error_types]\n",
    "    print('\\nResults:')\n",
    "    print('mean train rmse', np.mean(fold_errors, axis=0)[0])\n",
    "    print('mean test rmse', np.mean(fold_errors, axis=0)[1])\n",
    "    print('mean train mae', np.mean(fold_errors, axis=0)[2])\n",
    "    print('mean test mae', np.mean(fold_errors, axis=0)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing naive model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive model 1...\n",
      "\n",
      "Results:\n",
      "mean train rmse 1.1171010587983956\n",
      "mean test rmse 1.1171014505284547\n",
      "mean train mae 0.9338605988479152\n",
      "mean test mae 0.9338619186564217\n",
      "Run time:  3.7883293628692627  sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_naive_model(data, model_type='1')\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Run time: ', run_time, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing naive model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive model 2...\n",
      "\n",
      "Results:\n",
      "mean train rmse 1.0276727444564144\n",
      "mean test rmse 1.0354800404316467\n",
      "mean train mae 0.8227192335238153\n",
      "mean test mae 0.8289498348484955\n",
      "Run time:  13.37252926826477  sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_naive_model(data, model_type='2')\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Run time: ', run_time, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing naive model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive model 3...\n",
      "\n",
      "Results:\n",
      "mean train rmse 0.9742283446389879\n",
      "mean test rmse 0.9793666761448836\n",
      "mean train mae 0.7783363352007416\n",
      "mean test mae 0.7822841972440548\n",
      "Run time:  10.95653223991394  sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_naive_model(data, model_type='3')\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Run time: ', run_time, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix factorization\n",
    "\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_fold_error(ratings, train_indices, test_indices, users, movies, factors, n_training_steps, lr, lam):\n",
    "    \"\"\"\n",
    "    For a given fold, train the UM matrix and keep track of the rating errors on the train and test set.\n",
    "    \n",
    "    ratings: Dataframe, dataframe containing the ratings\n",
    "    train_indices: array, indices of the training set\n",
    "    test_indices array, indices of the test set\n",
    "    users: Dataframe, dataframe containing the users\n",
    "    movies: Dataframe, dataframe containing the movies\n",
    "    factors: int, the number of factors to use for the U and M matrix\n",
    "    n_training_steps: int, the number of steps for training\n",
    "    lr: float, learning rate \n",
    "    lam: float, lambda (used for regularization)\n",
    "    \"\"\"\n",
    "    n_users = len(users)\n",
    "    n_movies = len(movies)\n",
    "    \n",
    "    U = np.random.normal(0.0, 1.0, ((n_users, factors)))\n",
    "    M = np.random.normal(0.0, 1.0, ((factors, n_movies)))\n",
    "    \n",
    "    train_set = ratings.iloc[train_indices]\n",
    "    test_set = ratings.iloc[test_indices]\n",
    "    \n",
    "    # Define an index for every user ID\n",
    "    train_user_indices = train_set['UserID'].values - 1\n",
    "    test_user_indices = test_set['UserID'].values - 1\n",
    "    \n",
    "    # Define an index for every movie ID\n",
    "    movie_ids = movies['MovieID'].values\n",
    "    train_movie_indices = make_movie_indices(train_set, movie_ids)\n",
    "    test_movie_indices = make_movie_indices(test_set, movie_ids)\n",
    "    \n",
    "    # Create a matrix for looking up train set ratings\n",
    "    train_set_rating_matrix = np.zeros((n_users, n_movies))\n",
    "    for u, m, rating in zip(train_user_indices, train_movie_indices, train_set['Rating']):\n",
    "        train_set_rating_matrix[u, m] = rating\n",
    "    \n",
    "    errors = np.empty((0, 4))\n",
    "\n",
    "    for step in range(n_training_steps):\n",
    "        # Calculate the errors of the ratings predicted by the dot product of U and M\n",
    "        predicted_ratings = np.dot(U, M)\n",
    "\n",
    "        train_set_predicted_ratings = predicted_ratings[train_user_indices, train_movie_indices]\n",
    "        test_set_predicted_ratings = predicted_ratings[test_user_indices, test_movie_indices]\n",
    "\n",
    "        train_errors = train_set['Rating'] - crop_ratings(train_set_predicted_ratings)\n",
    "        test_errors = test_set['Rating'] - crop_ratings(test_set_predicted_ratings)\n",
    "        \n",
    "        train_rmse, test_rmse = rmse(train_errors), rmse(test_errors)\n",
    "        \n",
    "        errors = np.append(errors, np.array([[train_rmse, \n",
    "                                              test_rmse, \n",
    "                                              mae(train_errors), \n",
    "                                              mae(test_errors)]]), axis=0)\n",
    "\n",
    "        mp_lock.acquire()\n",
    "        print('step ', step, ' rmse train ', train_rmse, ' rmse test ', test_rmse)\n",
    "        mp_lock.release()\n",
    "        \n",
    "        # Update the U and M matrix\n",
    "        for u, m in zip(train_user_indices, train_movie_indices):\n",
    "            u_vec, m_vec = U[u, :], M[:, m]\n",
    "            error = train_set_rating_matrix[u, m] - np.dot(u_vec, m_vec)\n",
    "            U[u, :] += lr * (2 * error * m_vec - lam * u_vec)\n",
    "            M[:, m] += lr * (2 * error * U[u, :] - lam * m_vec)\n",
    "    \n",
    "    # errors has shape [n_training_steps, n_error_types]\n",
    "    return errors\n",
    "\n",
    "\n",
    "def matrix_factorization(data, factors, n_training_steps, lr, lam, n_folds):\n",
    "    \"\"\"\n",
    "    Calculates the error for the matrix factorization model. Bias in training and test set \n",
    "    selection is reduced by using cross-validation. To speed up the process, we use multiprocessing \n",
    "    to divide the folds over different cores.\n",
    "    \n",
    "    data: tuple, contains the user, movie and rating dataframe\n",
    "    factors: int, the number of factors to use for the U and M matrix\n",
    "    n_training_steps: int, the number of steps for training\n",
    "    lr: float, learning rate \n",
    "    lam: float, lambda (used for regularization)\n",
    "    n_folds: int, the number of folds to use for cross-validation\n",
    "    \"\"\"\n",
    "    users, movies, ratings = data\n",
    "\n",
    "    cv = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n",
    "\n",
    "    print('Testing matrix factorization model...')\n",
    "    \n",
    "    # Simulatneously calculate the errors for each fold\n",
    "    params = [(ratings,\n",
    "               train_indices, \n",
    "               test_indices, \n",
    "               users, \n",
    "               movies, \n",
    "               factors, \n",
    "               n_training_steps, \n",
    "               lr, \n",
    "               lam) for train_indices, test_indices in cv.split(ratings)]\n",
    "    pool = Pool(n_folds)\n",
    "    fold_errors = pool.starmap(mf_fold_error, params)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # fold_errors has dimensions [n_folds, n_training_steps, n_error_types]\n",
    "    return np.transpose(np.array(fold_errors), (0, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing matrix factorization model...\n",
      "step  0  rmse train  2.5019508435204645  rmse test  2.505985745810919\n",
      "step  0  rmse train  2.502395730645005  rmse test  2.504208276463159\n",
      "step  0  rmse train  2.5036820837908937  rmse test  2.499059980936231\n",
      "step  0  rmse train  2.50312504692612  rmse test  2.5012910024548005\n",
      "step  0  rmse train  2.502637664414911  rmse test  2.50324101250093\n",
      "step  1  rmse train  1.378383429890161.406812417182056  rmse test  \n",
      "step  1  rmse train  1.3855486453868322  rmse test  1.4206325506612607\n",
      "step  1  rmse train  1.374578568091938  rmse test  1.4082948269287678\n",
      "step  1  rmse train  1.3784721322705848  rmse test  1.4177585161228956\n",
      "step  1  rmse train  1.3837881339852656  rmse test  1.4131832873445507\n",
      "step  2  rmse train  0.9847830519497771  rmse test  1.0361493841791471\n",
      "step  2  rmse train  0.9839282514083296  rmse test  1.033250925357526\n",
      "step  2  rmse train  0.9809674959677085  rmse test  1.0385716351252674\n",
      "step  2  rmse train  0.9819292094912271  rmse test  1.0369683475652192\n",
      "step  2  rmse train  0.9858098279682455  rmse test  1.0369542545872932\n",
      "step  3  rmse train  0.9290076603691271  rmse test  0.9884618958391188\n",
      "step  3  rmse train  0.9305463152196427  rmse test  0.983280430587399\n",
      "step  3  rmse train  0.929710547300009  rmse test  0.985066481161517\n",
      "step  3  rmse train  0.930762893585608  rmse test  0.9823135858423527\n",
      "step  3  rmse train  0.930306702598685  rmse test  0.9832810745880898\n",
      "step  4  rmse train  0.9135578179965115  rmse test  0.9740983482195915\n",
      "step  4  rmse train  0.9150069032769691  rmse test  0.96831763922427\n",
      "step  4  rmse train  0.9152308196060934  rmse test  0.9680410647912406\n",
      "step  4  rmse train  0.914274833697723  rmse test  0.9704460668000978\n",
      "step  4  rmse train  0.9145556489188115  rmse test  0.9688057349382803\n",
      "step  5  rmse train  0.9062874448105341  rmse test  0.9603471491007659\n",
      "step  5  rmse train  0.9053396330952982  rmse test  0.9624616962463629\n",
      "step  5  rmse train  0.9055861818573575  rmse test  0.960989150192394\n",
      "step  5  rmse train  0.9060941598404336  rmse test  0.9601984760512648\n",
      "step  5  rmse train  0.9046395576017033  rmse test  0.9662250108510909\n",
      "step  6  rmse train  0.8992208178201538  rmse test  0.9545537322275609\n",
      "step  6  rmse train  0.8976038975536463  rmse test  0.9602665125941556\n",
      "step  6  rmse train  0.8982807744237764  rmse test  0.9564574413720107\n",
      "step  6  rmse train  0.8990626203909804  rmse test  0.9541162783124605\n",
      "step  6  rmse train  0.8985408529003018  rmse test  0.9551956439121764\n",
      "step  7  rmse train  0.8929090155858135  rmse test  0.949621668368682\n",
      "step  7  rmse train  0.8913751707864477  rmse test  0.9552117755689162\n",
      "step  7  rmse train  0.8927803389631302  rmse test  0.9489612728006166\n",
      "step  7  rmse train  0.8923095679802406  rmse test  0.9502600603759428\n",
      "step  7  rmse train  0.892017253558824  rmse test  0.951357993244356\n",
      "step  8  rmse train  0.8870551784982043  rmse test  0.9451930322964379\n",
      "step  8  rmse train  0.8856447384238718  rmse test  0.9507261575207068\n",
      "step  8  rmse train  0.8869126068421453  rmse test  0.9443330780646747\n",
      "step  8  rmse train  0.8862550910312715  rmse test  0.9468331626383102\n",
      "step  8  rmse train  0.886545881064795  rmse test  0.9458177184911519\n",
      "step  9  rmse train  0.8816433086573588  rmse test  0.9412151928109982\n",
      "step  9  rmse train  0.8803607097376444  rmse test  0.9467033093837052\n",
      "step  9  rmse train  0.881206481917309  rmse test  0.9418063382500953\n",
      "step  9  rmse train  0.881456145169971  rmse test  0.9401706482636811\n",
      "step  9  rmse train  0.8809549624596381  rmse test  0.9428016400065523\n",
      "step  10  rmse train  0.8766797284936598  rmse test  0.9376717114709938\n",
      "step  10  rmse train  0.8755050920677822  rmse test  0.9431221679130387\n",
      "step  10  rmse train  0.87629195971091  rmse test  0.9382022421818265\n",
      "step  10  rmse train  0.8764310943714789  rmse test  0.9364512693284804\n",
      "step  10  rmse train  0.8760975621788334  rmse test  0.9392230688024276\n",
      "step  11  rmse train  0.8721219747205492  rmse test  0.934526247542474\n",
      "step  11  rmse train  0.8710312124325431  rmse test  0.939939423804354\n",
      "step  11  rmse train  0.8717663862860021  rmse test  0.9349821981666634\n",
      "step  11  rmse train  0.871805729617053  rmse test  0.9331337649802438\n",
      "step  11  rmse train  0.871637090724293  rmse test  0.9360400124068399\n",
      "step  12  rmse train  0.8679033982954149  rmse test  0.9317127510724518\n",
      "step  12  rmse train  0.8668718486093836  rmse test  0.9370917485376536\n",
      "step  12  rmse train  0.8675642234932753  rmse test  0.9320891704490594\n",
      "step  12  rmse train  0.8675188712727161  rmse test  0.9301727596541854\n",
      "step  12  rmse train  0.8675027050767193  rmse test  0.9331926136866635\n",
      "step  13  rmse train  0.8639498856373852  rmse test  0.9291557549875734\n",
      "step  13  rmse train  0.8636148640358847  rmse test  0.9294528316800177\n",
      "step  13  rmse train  0.8629655616881065  rmse test  0.9345046033181448\n",
      "step  13  rmse train  0.8635008523944399  rmse test  0.9274882914692745\n",
      "step  13  rmse train  0.8636249002532099  rmse test  0.9306191807299394\n",
      "step  14  rmse train  0.8602026626542593  rmse test  0.9268004181550681\n",
      "step  14  rmse train  0.8592610452875975  rmse test  0.9321204366957918\n",
      "step  14  rmse train  0.8597020778519917  rmse test  0.9250267262534244\n",
      "step  14  rmse train  0.859863064675271  rmse test  0.9270062446803113\n",
      "step  14  rmse train  0.8599449993957863  rmse test  0.9282500652007475\n",
      "step  15  rmse train  0.8566180857982034  rmse test  0.9246021633009819\n",
      "step  15  rmse train  0.8557257208269138  rmse test  0.9299004817593066\n",
      "step  15  rmse train  0.8560810843644976  rmse test  0.9227370076210577\n",
      "step  15  rmse train  0.8562696450595036  rmse test  0.9247071436071084\n",
      "step  15  rmse train  0.8564223852072749  rmse test  0.9260389882103867\n",
      "step  16  rmse train  0.8523379203351562  rmse test  0.9278063726098958\n",
      "step  16  rmse train  0.853165016078187  rmse test  0.9225249841213563\n",
      "step  16  rmse train  0.8526123462135434  rmse test  0.9205973921346167\n",
      "step  16  rmse train  0.8528098402095984  rmse test  0.922543394116267\n",
      "step  16  rmse train  0.8530291808123859  rmse test  0.9239557790739532\n",
      "step  17  rmse train  0.849085048554105  rmse test  0.9258146920770498\n",
      "step  17  rmse train  0.8492799184398717  rmse test  0.9185809170777974\n",
      "step  17  rmse train  0.8498272265614389  rmse test  0.9205434831782218\n",
      "step  17  rmse train  0.8494700279661003  rmse test  0.9204939668015671\n",
      "step  17  rmse train  0.8497514530503217  rmse test  0.9219762806125952\n",
      "step  18  rmse train  0.8460747243501585  rmse test  0.916672938199007\n",
      "step  18  rmse train  0.8459611943657163  rmse test  0.9239169550577246\n",
      "step  18  rmse train  0.8466010617687688  rmse test  0.9186563989967275\n",
      "step  18  rmse train  0.8462469828749454  rmse test  0.9185471830936138\n",
      "step  18  rmse train  0.8465841766277042  rmse test  0.9200856038759654\n",
      "step  19  rmse train  0.8429915295833256  rmse test  0.914854745444892\n",
      "step  19  rmse train  0.8429641681834668  rmse test  0.9221028863884062\n",
      "step  19  rmse train  0.8434872938536168  rmse test  0.9168628535370645\n",
      "step  19  rmse train  0.8431410206136679  rmse test  0.9166926862499809\n",
      "step  19  rmse train  0.8435285161961984  rmse test  0.9182667047101577\n",
      "step  20  rmse train  0.8400294000224878  rmse test  0.9131257595651004\n",
      "step  20  rmse train  0.8400928994155478  rmse test  0.9203726655545392\n",
      "step  20  rmse train  0.8404896739598774  rmse test  0.9151530353757078\n",
      "step  20  rmse train  0.8401548558224988  rmse test  0.9149328888181584\n",
      "step  20  rmse train  0.8405871338079298  rmse test  0.9165264007661474\n",
      "step  21  rmse train  0.8371879135237671  rmse test  0.9114870914938835\n",
      "step  21  rmse train  0.8373448076817552  rmse test  0.9187276149631947\n",
      "step  21  rmse train  0.8372905979181114  rmse test  0.9132540138808152\n",
      "step  21  rmse train  0.8376121036900048  rmse test  0.9135252822921316\n",
      "step  21  rmse train  0.8377615194444106  rmse test  0.9148679948067271\n",
      "step  22  rmse train  0.8344657550147302  rmse test  0.9099378768422238\n",
      "step  22  rmse train  0.834717251122772  rmse test  0.917162344248638\n",
      "step  22  rmse train  0.8345490005827167  rmse test  0.9116682877671293\n",
      "step  22  rmse train  0.8348562149652816  rmse test  0.911979657848141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  22  rmse train  0.8350517004109062  rmse test  0.9132855992928145\n",
      "step  23  rmse train  0.8318617212963242  rmse test  0.9084726902026825\n",
      "step  23  rmse train  0.8322080088287448  rmse test  0.9156725726872004\n",
      "step  23  rmse train  0.8319287154898657  rmse test  0.9101692093737196\n",
      "step  23  rmse train  0.8322224594355224  rmse test  0.9105188335034032\n",
      "step  23  rmse train  0.8324579497132251  rmse test  0.9117802367204397\n",
      "step  24  rmse train  0.8293730333823  rmse test  0.9070806090216235\n",
      "step  24  rmse train  0.8298131671733846  rmse test  0.9142612817658045\n",
      "step  24  rmse train  0.8297102704859431  rmse test  0.9091313459800147\n",
      "step  24  rmse train  0.8294275470655599  rmse test  0.9087523724943434\n",
      "step  24  rmse train  0.8299772849344755  rmse test  0.9103548132675923\n",
      "step  25  rmse train  0.826996369297587  rmse test  0.9057658923809384\n",
      "step  25  rmse train  0.8275282399806091  rmse test  0.9129242199335552\n",
      "step  25  rmse train  0.8273156907267926  rmse test  0.9078191644548212\n",
      "step  25  rmse train  0.8270417930383839  rmse test  0.9074157465630946\n",
      "step  25  rmse train  0.8276059390592171  rmse test  0.9090049853642491\n",
      "step  26  rmse train  0.8247280302887912  rmse test  0.9045163714966238\n",
      "step  26  rmse train  0.825347645895401  rmse test  0.9116552757447836\n",
      "step  26  rmse train  0.8247665919027454  rmse test  0.90615205861984\n",
      "step  26  rmse train  0.8250355299624251  rmse test  0.9065826907832254\n",
      "step  26  rmse train  0.8253390791220232  rmse test  0.9077253469936503\n",
      "step  27  rmse train  0.8232664413578976  rmse test  0.910451938703144\n",
      "step  27  rmse train  0.8225639470513605  rmse test  0.903332395734718\n",
      "step  27  rmse train  0.8228645533330926  rmse test  0.9054102580384187\n",
      "step  27  rmse train  0.8225970094113512  rmse test  0.9049568434620002\n",
      "step  27  rmse train  0.8231730388332218  rmse test  0.9065105107158232\n",
      "step  28  rmse train  0.8204993175925569   rmse test 0.9022106142906768\n",
      "step  28  rmse train  0.8212794877261609  rmse test  0.9093133168497728\n",
      "step  28  rmse train  0.820798523890504  rmse test  0.9043018482214211\n",
      "step  28  rmse train  0.8205281795684325  rmse test  0.9038276821395002\n",
      "step  28  rmse train  0.8211031084537895  rmse test  0.9053593057157592\n",
      "step  29  rmse train  0.8185291574546416  rmse test  0.901148036012111\n",
      "step  29  rmse train  0.8188317132384749  rmse test  0.9032527605462535\n",
      "step  29  rmse train  0.8193818894587928  rmse test  0.90823599658891\n",
      "step  29  rmse train  0.8185557511242367  rmse test  0.9027587496956374\n",
      "step  29  rmse train  0.8191237984174791  rmse test  0.9042701530960375\n",
      "step  30  rmse train  0.8166489789098146  rmse test  0.9001433279008902\n",
      "step  30  rmse train  0.8169592476286806  rmse test  0.9022638918577083\n",
      "step  30  rmse train  0.8175683163275216  rmse test  0.9072136389210772\n",
      "step  30  rmse train  0.8166737520336455  rmse test  0.9017441693188671\n",
      "step  30  rmse train  0.8172307776862822  rmse test  0.9032355361605799\n",
      "step  31  rmse train  0.8148535181575702  rmse test  0.8991889327298507\n",
      "step  31  rmse train  0.8151753647786025  rmse test  0.9013291241063704\n",
      "step  31  rmse train  0.8158343534985139  rmse test  0.9062408936162881\n",
      "step  31  rmse train  0.814877344870425  rmse test  0.9007814656886501\n",
      "step  31  rmse train  0.8154190678648402  rmse test  0.9022579290624548\n",
      "step  32  rmse train  0.8134753275568163  rmse test  0.9004437638552727\n",
      "step  32  rmse train  0.8131382601216481  rmse test  0.8982839789926388\n",
      "step  32  rmse train  0.814175791709024  rmse test  0.905315489124702\n",
      "step  32  rmse train  0.8131617887116971  rmse test  0.8998671696879429\n",
      "step  32  rmse train  0.8136856255767766  rmse test  0.9013301566134867\n",
      "step  33  rmse train  0.8118548535965395  rmse test  0.8996042248510424\n",
      "step  33  rmse train  0.8114988994134548  rmse test  0.8974221809871219\n",
      "step  33  rmse train  0.8125885778812142  rmse test  0.9044340396961865\n",
      "step  33  rmse train  0.8115226976140736  rmse test  0.8989981979706884\n",
      "step  33  rmse train  0.8120266356725192  rmse test  0.9004500292289077\n",
      "step  34  rmse train  0.8103094048933998  rmse test  0.8988068750836578\n",
      "step  34  rmse train  0.8099311679302624  rmse test  0.8966036123445461\n",
      "step  34  rmse train  0.8110687348888764  rmse test  0.9035981575743617\n",
      "step  34  rmse train  0.8099554155095303  rmse test  0.8981726284875243\n",
      "step  34  rmse train  0.8104382634853915  rmse test  0.8996143799751783\n",
      "step  35  rmse train  0.8088346400583941  rmse test  0.8980487141017447\n",
      "step  35  rmse train  0.8084306759589183  rmse test  0.895825532299086\n",
      "step  35  rmse train  0.8096121409718099  rmse test  0.902803761332151\n",
      "step  35  rmse train  0.8084560989723846  rmse test  0.8973882978894082\n",
      "step  35  rmse train  0.8089174877262644  rmse test  0.8988221957892265\n",
      "step  36  rmse train  0.8074262284275534  rmse test  0.8973285709730391\n",
      "step  36  rmse train  0.8069940109195521  rmse test  0.895085806936253\n",
      "step  36  rmse train  0.8082155340211741  rmse test  0.902045315633381\n",
      "step  36  rmse train  0.8074614089658452  rmse test  0.8980694605411865\n",
      "step  36  rmse train  0.8070210840354067  rmse test  0.8966395129235498\n",
      "step  37  rmse train  0.8060807195918074  rmse test  0.8966437132528471\n",
      "step  37  rmse train  0.8056174831475432  rmse test  0.8943806594612825\n",
      "step  37  rmse train  0.8068753237313451  rmse test  0.90132321845368\n",
      "step  37  rmse train  0.8060672336209812  rmse test  0.8973532989646288\n",
      "step  37  rmse train  0.805646512957772  rmse test  0.8959251790232992\n",
      "step  38  rmse train  0.804794367619628  rmse test  0.8959919495383017\n",
      "step  38  rmse train  0.8042979102997152  rmse test  0.8937082092706524\n",
      "step  38  rmse train  0.8055885554828587  rmse test  0.9006342016501803\n",
      "step  38  rmse train  0.804329083720311  rmse test  0.8952443831603235\n",
      "step  38  rmse train  0.8047315327736193  rmse test  0.8966748040427901\n",
      "step  39  rmse train  0.8035641569721875  rmse test  0.8953717579173129\n",
      "step  39  rmse train  0.8030324928842022  rmse test  0.8930682278878587\n",
      "step  39  rmse train  0.8043524574235107  rmse test  0.8999754035120147\n",
      "step  39  rmse train  0.8030657311190066  rmse test  0.8945946700607241\n",
      "step  39  rmse train  0.8034513583651866  rmse test  0.8960315554623665\n",
      "step  40  rmse train  0.8023870116260547  rmse test  0.8947793682679116\n",
      "step  40  rmse train  0.8018180818100854  rmse test  0.8924591255814035\n",
      "step  40  rmse train  0.8031645395101973  rmse test  0.8993420060779787\n",
      "step  40  rmse train  0.8022244719405066  rmse test  0.8954182636900496\n",
      "step  40  rmse train  0.8018533954262875  rmse test  0.8939724613929612\n",
      "step  41  rmse train  0.800652012916187  rmse test  0.8918775465899952\n",
      "step  41  rmse train  0.8012600428979345  rmse test  0.8942166595126669\n",
      "step  41  rmse train  0.8020222268432529  rmse test  0.898734174728948\n",
      "step  41  rmse train  0.8010484220647524  rmse test  0.8948344286157762\n",
      "step  41  rmse train  0.8006897413231987  rmse test  0.8933745531205516\n",
      "step  42  rmse train  0.7995321932670085  rmse test  0.8913216993064061\n",
      "step  42  rmse train  0.8001806403621264  rmse test  0.8936812221726838\n",
      "step  42  rmse train  0.8009232679138493  rmse test  0.8981499079815622\n",
      "step  42  rmse train  0.7999204291497025  rmse test  0.8942806594187349\n",
      "step  42  rmse train  0.7995722183444679  rmse test  0.8928009883016885\n",
      "step  43  rmse train  0.7984565672399105  rmse test  0.8907899445029235\n",
      "step  43  rmse train  0.7991464260697496  rmse test  0.8931707560189099\n",
      "step  43  rmse train  0.7998656131926293  rmse test  0.8975892734274358\n",
      "step  43  rmse train  0.7988386859670411  rmse test  0.8937557935681696\n",
      "step  43  rmse train  0.7984988067788189  rmse test  0.8922500295867765\n",
      "step  44  rmse train  0.7974229866632327  rmse test  0.890283472880529\n",
      "step  44  rmse train  0.798154949892106  rmse test  0.8926841206653351\n",
      "step  44  rmse train  0.7988469579050226  rmse test  0.8970482234553401\n",
      "step  44  rmse train  0.7978008243137545  rmse test  0.8932572093479348\n",
      "step  44  rmse train  0.7974674419360359  rmse test  0.8917186040808621\n",
      "step  45  rmse train  0.7964295647663734  rmse test  0.8898001549816863\n",
      "step  45  rmse train  0.7972037585181024  rmse test  0.8922197913902212\n",
      "step  45  rmse train  0.7978655761496588  rmse test  0.8965253982644492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  45  rmse train  0.7968045595934697  rmse test  0.8927830431284217\n",
      "step  45  rmse train  0.7964760026516021  rmse test  0.8912065008694243\n",
      "step  46  rmse train  0.7954744008397058  rmse test  0.8893388743630711\n",
      "step  46  rmse train  0.7962908379331431  rmse test  0.891776226613867\n",
      "step  46  rmse train  0.7969198297799978  rmse test  0.8960216582005549\n",
      "step  46  rmse train  0.795847388077164  rmse test  0.8923309821280001\n",
      "step  46  rmse train  0.7955227293599264  rmse test  0.8907121649091126\n",
      "step  47  rmse train  0.7945555066743207  rmse test  0.8888973430361786\n",
      "step  47  rmse train  0.7954142298384029  rmse test  0.8913517067960152\n",
      "step  47  rmse train  0.7960080918676763  rmse test  0.8955339235359496\n",
      "step  47  rmse train  0.7949277444230625  rmse test  0.8918974075045495\n",
      "step  47  rmse train  0.7946058392971822  rmse test  0.8902331425742291\n",
      "step  48  rmse train  0.7936715600002023  rmse test  0.8884750971342698\n",
      "step  48  rmse train  0.7945722112251163  rmse test  0.8909461368713645\n",
      "step  48  rmse train  0.7951287434030115  rmse test  0.8950621423214817\n",
      "step  48  rmse train  0.7940437685975995  rmse test  0.8914821862969841\n",
      "step  48  rmse train  0.7937237756939209  rmse test  0.8897723033920493\n",
      "step  49  rmse train  0.7928210128743938  rmse test  0.8880695371692351\n",
      "step  49  rmse train  0.7937631756436557  rmse test  0.8905569748247627\n",
      "step  49  rmse train  0.7942802465330167  rmse test  0.8946069919222998\n",
      "step  49  rmse train  0.7931939371562173  rmse test  0.8910847259356554\n",
      "step  49  rmse train  0.792874845695106  rmse test  0.8893296144074825\n",
      "step  50  rmse train  0.7920022228083475  rmse test  0.8876801029961937\n",
      "step  50  rmse train  0.7929854922177446  rmse test  0.8901832470420217\n",
      "step  50  rmse train  0.7934613941385917  rmse test  0.8941673843470079\n",
      "step  50  rmse train  0.7923764842236867  rmse test  0.8907043678698359\n",
      "step  50  rmse train  0.7920576377893966  rmse test  0.8889015114101392\n",
      "step  51  rmse train  0.7912139558601762  rmse test  0.8873057296078842\n",
      "step  51  rmse train  0.792237703507753  rmse test  0.889825768205391\n",
      "step  51  rmse train  0.7926709081939186  rmse test  0.8937417716202788\n",
      "step  51  rmse train  0.7915899916755968  rmse test  0.8903400531496313\n",
      "step  51  rmse train  0.7912708985425745  rmse test  0.8884874483665876\n",
      "step  52  rmse train  0.790454970731263  rmse test  0.8869455755691994\n",
      "step  52  rmse train  0.7915182634939939  rmse test  0.8894834351867756\n",
      "step  52  rmse train  0.7919075559896324  rmse test  0.8933287563935376\n",
      "step  52  rmse train  0.7908329418406287  rmse test  0.8899904281346512\n",
      "step  52  rmse train  0.7905132397862411  rmse test  0.8880876881772238\n",
      "step  53  rmse train  0.7897240517946782  rmse test  0.8865978449271348\n",
      "step  53  rmse train  0.7908257204584764  rmse test  0.8891543528539447\n",
      "step  53  rmse train  0.7911702287195749  rmse test  0.892929603246024\n",
      "step  53  rmse train  0.790103963501709  rmse test  0.8896539544819553\n",
      "step  53  rmse train  0.7897836931448748  rmse test  0.8877017900647937\n",
      "step  54  rmse train  0.7890199004258102  rmse test  0.8862623768675709\n",
      "step  54  rmse train  0.790158990577159  rmse test  0.8888394649679235\n",
      "step  54  rmse train  0.7904578560707912  rmse test  0.8925418779092342\n",
      "step  54  rmse train  0.7894017479544077  rmse test  0.8893295947881469\n",
      "step  54  rmse train  0.7890810459168507  rmse test  0.8873287441503388\n",
      "step  55  rmse train  0.78834132264623  rmse test  0.8859387760591241\n",
      "step  55  rmse train  0.7895167914964353  rmse test  0.8885382151748916\n",
      "step  55  rmse train  0.7897695110117027  rmse test  0.8921663490331129\n",
      "step  55  rmse train  0.7887248932853912  rmse test  0.8890178716284975\n",
      "step  55  rmse train  0.7884040200337705  rmse test  0.8869682945189655\n",
      "step  56  rmse train  0.7876873557603385  rmse test  0.8856280579064494\n",
      "step  56  rmse train  0.7888980065259877  rmse test  0.8882508219304364\n",
      "step  56  rmse train  0.7891039514796419  rmse test  0.8918018524827757\n",
      "step  56  rmse train  0.7877516733885355  rmse test  0.8866206698870048\n",
      "step  56  rmse train  0.7880722924859681  rmse test  0.8887171620499191\n",
      "step  57  rmse train  0.7870571027922793  rmse test  0.8853295537925311\n",
      "step  57  rmse train  0.7883015545888683  rmse test  0.887975882077794\n",
      "step  57  rmse train  0.788460193324972  rmse test  0.8914459089352602\n",
      "step  57  rmse train  0.7874426994509589  rmse test  0.8884277082913198\n",
      "step  57  rmse train  0.7871230535347286  rmse test  0.8862847814419864\n",
      "step  58  rmse train  0.7864496100480448  rmse test  0.8850423513787969\n",
      "step  58  rmse train  0.7877263670275363  rmse test  0.8877124447587366\n",
      "step  58  rmse train  0.7878375726321876  rmse test  0.8910989056182489\n",
      "step  58  rmse train  0.7868350685137114  rmse test  0.8881475075157031\n",
      "step  58  rmse train  0.7865170600888665  rmse test  0.8859601223741401\n",
      "step  59  rmse train  0.7858639210723048  rmse test  0.8847653834323781\n",
      "step  59  rmse train  0.7871714065899262  rmse test  0.887458660801462\n",
      "step  59  rmse train  0.7872353623106956  rmse test  0.8907611082135122\n",
      "step  59  rmse train  0.7862484554402065  rmse test  0.8878770373137299\n",
      "step  59  rmse train  0.7859327450287494  rmse test  0.885646743120121\n",
      "step  60  rmse train  0.7852991068100134  rmse test  0.8844997120008461\n",
      "step  60  rmse train  0.7866358611731892  rmse test  0.8872157300680414\n",
      "step  60  rmse train  0.7866527602529745  rmse test  0.8904320943080112\n",
      "step  60  rmse train  0.785681786816191  rmse test  0.8876141367332191\n",
      "step  60  rmse train  0.7853693932826242  rmse test  0.8853425425227097\n",
      "step  61  rmse train  0.7847542670596553  rmse test  0.884244541469796\n",
      "step  61  rmse train  0.7861188427801338  rmse test  0.8869818233260758\n",
      "step  61  rmse train  0.7860889850464033  rmse test  0.890112123480441\n",
      "step  61  rmse train  0.7851343190273975  rmse test  0.8873581424109961\n",
      "step  61  rmse train  0.7848259361090971  rmse test  0.8850488235868522\n",
      "step  62  rmse train  0.7842285385203127  rmse test  0.8839994829104537\n",
      "step  62  rmse train  0.785619545795932  rmse test  0.8867559635003256\n",
      "step  62  rmse train  0.7855434485859146  rmse test  0.8898022206986209\n",
      "step  62  rmse train  0.7846051666778959  rmse test  0.8871109930815773\n",
      "step  62  rmse train  0.784301604835544  rmse test  0.8847645491041531\n",
      "step  63  rmse train  0.7837210141146354  rmse test  0.8837637624310112\n",
      "step  63  rmse train  0.7851369806847555  rmse test  0.886538666058117\n",
      "step  63  rmse train  0.7850154068455633  rmse test  0.8895014391303494\n",
      "step  63  rmse train  0.7840934421634057  rmse test  0.8868719197893635\n",
      "step  63  rmse train  0.7837956057901866  rmse test  0.8844890437046281\n",
      "step  64  rmse train  0.7832309340490539  rmse test  0.8835363393690937\n",
      "step  64  rmse train  0.7846704387912227  rmse test  0.8863295053873909\n",
      "step  64  rmse train  0.7845042689888259  rmse test  0.8892098129862327\n",
      "step  64  rmse train  0.7835982902772317  rmse test  0.8866407711275738\n",
      "step  64  rmse train  0.78330715624781  rmse test  0.8842217622686138\n",
      "step  65  rmse train  0.7827576313911585  rmse test  0.8833161982224347\n",
      "step  65  rmse train  0.784219210240281  rmse test  0.8861289788288964\n",
      "step  65  rmse train  0.7840093455919543  rmse test  0.8889270523607324\n",
      "step  65  rmse train  0.7831190098982156  rmse test  0.8864170621855711\n",
      "step  65  rmse train  0.78283551648689  rmse test  0.8839641468087769\n",
      "step  66  rmse train  0.78230048101886  rmse test  0.8831038222197953\n",
      "step  66  rmse train  0.7837826980856117  rmse test  0.8859367209914856\n",
      "step  66  rmse train  0.7835301031339517  rmse test  0.8886522929950658\n",
      "step  66  rmse train  0.7826550860463409  rmse test  0.886200501712756\n",
      "step  66  rmse train  0.7823800087711029  rmse test  0.8837153806795667\n",
      "step  67  rmse train  0.7818588193270845  rmse test  0.8828988630340893\n",
      "step  67  rmse train  0.7833602014328829  rmse test  0.8857523832099902\n",
      "step  67  rmse train  0.783066061374173  rmse test  0.8883855721306833\n",
      "step  67  rmse train  0.7822057604736407  rmse test  0.8859901801166815\n",
      "step  67  rmse train  0.7819399262407549  rmse test  0.883475159009828\n",
      "step  68  rmse train  0.7814319127942965  rmse test  0.8827004908702835\n",
      "step  68  rmse train  0.7829511764532475  rmse test  0.8855744247938143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  68  rmse train  0.7826166042716771  rmse test  0.8881264170482555\n",
      "step  68  rmse train  0.7817705241878579  rmse test  0.8857868069499427\n",
      "step  68  rmse train  0.7815145868373561  rmse test  0.8832432291037693\n",
      "step  69  rmse train  0.7810191900255841  rmse test  0.8825094577184901\n",
      "step  69  rmse train  0.7825551154566688  rmse test  0.8854028596194108\n",
      "step  69  rmse train  0.7821812059189506  rmse test  0.8878754813093396\n",
      "step  69  rmse train  0.7813487231783487  rmse test  0.8855882284144281\n",
      "step  69  rmse train  0.7811033468602429  rmse test  0.8830185784466127\n",
      "step  70  rmse train  0.7806201109171953  rmse test  0.8823248012994216\n",
      "step  70  rmse train  0.7821713780161252  rmse test  0.8852376897277019\n",
      "step  70  rmse train  0.7817594293720553  rmse test  0.8876305292921574\n",
      "step  70  rmse train  0.7809397197878086  rmse test  0.8853945155956866\n",
      "step  70  rmse train  0.7807055556848921  rmse test  0.8828008959368389\n",
      "step  71  rmse train  0.7802341155520468  rmse test  0.8821463068931261\n",
      "step  71  rmse train  0.7817994738210973  rmse test  0.8850794788405599\n",
      "step  71  rmse train  0.7813506865597809  rmse test  0.8873929398371418\n",
      "step  71  rmse train  0.7805430925207689  rmse test  0.8852052601890501\n",
      "step  71  rmse train  0.7803205533136988  rmse test  0.88258967757697\n",
      "step  72  rmse train  0.7798605713529778  rmse test  0.8819740586300845\n",
      "step  72  rmse train  0.781438874793092  rmse test  0.8849270068926797\n",
      "step  72  rmse train  0.780954515360573  rmse test  0.8871624353938679\n",
      "step  72  rmse train  0.7801583223321771  rmse test  0.885021202590408\n",
      "step  72  rmse train  0.7799478633899404  rmse test  0.8823849214297391\n",
      "step  73  rmse train  0.7794990170733435  rmse test  0.8818078762017886\n",
      "step  73  rmse train  0.7810891050383597  rmse test  0.8847790052443522\n",
      "step  73  rmse train  0.7805705875676765  rmse test  0.8869385802657539\n",
      "step  73  rmse train  0.7797849280176667  rmse test  0.8848421349513124\n",
      "step  73  rmse train  0.7795869893622213  rmse test  0.8821862355966419\n",
      "step  74  rmse train  0.7791488895341309  rmse test  0.8816470179312734\n",
      "step  74  rmse train  0.7807498149253189  rmse test  0.8846356135558844\n",
      "step  74  rmse train  0.7801985007920069  rmse test  0.8867211699543331\n",
      "step  74  rmse train  0.7794224656952001  rmse test  0.8846675643435845\n",
      "step  74  rmse train  0.7792375292879732  rmse test  0.8819939605283554\n",
      "\n",
      "Results:\n",
      "Final train rmse:  0.779751440046926\n",
      "Final test rmse:  0.8839330652626861\n",
      "Final train mae:  0.6123021393959192\n",
      "Final test mae:  0.6893740464603942\n",
      "Run time:  835.9442446231842  sec\n"
     ]
    }
   ],
   "source": [
    "# Use a lock to prevent print statements from being scrambled\n",
    "mp_lock = Lock()\n",
    "\n",
    "number_of_folds = 5\n",
    "number_of_training_steps = 75\n",
    "number_of_factors = 10\n",
    "learning_rate = 0.005\n",
    "regularization = 0.05\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "errors = matrix_factorization(data,\n",
    "                              factors=number_of_factors,\n",
    "                              n_training_steps=number_of_training_steps,\n",
    "                              lr=learning_rate,\n",
    "                              lam=regularization,\n",
    "                              n_folds=number_of_folds)\n",
    "\n",
    "mean_train_rmses = np.mean(errors, axis=0)[0]\n",
    "mean_test_rmses = np.mean(errors, axis=0)[1]\n",
    "mean_train_maes = np.mean(errors, axis=0)[2]\n",
    "mean_test_maes = np.mean(errors, axis=0)[3]\n",
    "\n",
    "print('\\nResults:')\n",
    "print('Final train rmse: ', np.min(mean_train_rmses))\n",
    "print('Final test rmse: ', np.min(mean_test_rmses))\n",
    "print('Final train mae: ', np.min(mean_train_maes))\n",
    "print('Final test mae: ', np.min(mean_test_maes))\n",
    "\n",
    "run_time = time.time() - t0\n",
    "\n",
    "print('Run time: ', run_time, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_steps = np.arange(number_of_training_steps)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(training_steps, mean_train_rmses, label='train rmse', rasterized=True)\n",
    "ax.plot(training_steps, mean_test_rmses, label='test rmse', rasterized=True)\n",
    "ax.plot(training_steps, mean_train_maes, label='train mae', rasterized=True)\n",
    "ax.plot(training_steps, mean_test_maes, label='test mae', rasterized=True)\n",
    "ax.set_title('Matrix factorization errors')\n",
    "ax.set_xlabel('training step')\n",
    "ax.set_ylabel('error')\n",
    "ax.legend()\n",
    "plt.savefig('mf_errors.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
