{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from multiprocessing import Pool, Lock\n",
    "\n",
    "# Use a lock to prevent print statements from being scrambled\n",
    "mp_lock = Lock()\n",
    "\n",
    "data_dir = 'ml-1m'\n",
    "\n",
    "movies_filename = 'movies.dat'\n",
    "users_filename = 'users.dat'\n",
    "ratings_filename = 'ratings.dat'\n",
    "\n",
    "movie_columns = ['MovieID', 'Title', 'Genres']\n",
    "user_columns = ['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code']\n",
    "rating_columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    "\n",
    "\n",
    "def make_dataframe(data_dir, filename, columns):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from a data file\n",
    "    \n",
    "    data_dir: string, directory of data files\n",
    "    filename: string, data filename\n",
    "    columns: list, names for the columns of the dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    data_file = os.path.join(data_dir, filename)\n",
    "    return pd.read_csv(data_file, delimiter='::', names=columns, encoding='latin-1', engine='python')\n",
    "\n",
    "\n",
    "# Make the data frames for each data file\n",
    "movies = make_dataframe(data_dir, movies_filename, movie_columns)\n",
    "users = make_dataframe(data_dir, users_filename, user_columns)\n",
    "ratings = make_dataframe(data_dir, ratings_filename, rating_columns)\n",
    "data = (users, movies, ratings)\n",
    "\n",
    "    \n",
    "def rmse(errors):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error for an array of errors\n",
    "    \n",
    "    errors: array, array containing errors\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.square(errors)))\n",
    "\n",
    "\n",
    "def mae(errors):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute error for an array of errors\n",
    "    \n",
    "    errors: array, array containing errors\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(errors))\n",
    "\n",
    "\n",
    "def crop_ratings(arr):\n",
    "    \"\"\"\n",
    "    Ratings can only have values between 1.0 and 5.0. \n",
    "    This function sets all ratings > 5 to 5 and < 1 to 1.\n",
    "    \n",
    "    arr: array, array containing ratings\n",
    "    \"\"\"\n",
    "    new_arr = np.where(arr > 5.0, 5.0, arr)\n",
    "    new_arr = np.where(new_arr < 1.0, 1.0, arr)\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def rating_errors(data_set, model):\n",
    "    \"\"\"\n",
    "    Calculates the rmse and mae of the errors between ratings from and data set\n",
    "    and ratings predicted by a model.\n",
    "    \n",
    "    test_set: Dataframe, dataframe with the test data\n",
    "    model: function, function that can act on the rows of a dataframe\n",
    "    \"\"\"\n",
    "    errors = data_set['Rating'] - data_set.apply(model, axis=1)\n",
    "    return rmse(errors), mae(errors)\n",
    "\n",
    "\n",
    "def ids_to_indices(movie_ids):\n",
    "    \"\"\"\n",
    "    We want for each movie ID the corresponding index of an array. For the users this \n",
    "    is straightforward, since the the user ID's are integers from 1 to the number of \n",
    "    users and we can just subtract 1 to get all the indices. For some reason, some integers \n",
    "    are skipped in the movie ID's, so we cannot use them directly as indices of an array. \n",
    "    \n",
    "    For example: there is no movie with ID '91', so we want the movie with ID '92' \n",
    "    to correspond to an index of 90 (note that the first movie ID is '1' which \n",
    "    corresponds to index 0)\n",
    "    \n",
    "    Therefore we make an indices array with holes, which has parts like \n",
    "    [0, 1, ..., 218, 0, 219, 220, 221, ...]. The IDs '91' and '221' are missing. So a movie \n",
    "    with ID '222' will then take the 221st element of this array, which will take the value\n",
    "    219 for the index, because 2 zeros are inserted for the missing IDs.\n",
    "    \n",
    "    movie_ids: array, array containing the ID's of all movies\n",
    "    \"\"\"\n",
    "    ids_to_indices_arr = np.array([], dtype=np.int32)\n",
    "    skipped_integers = 0\n",
    "    for i in np.arange(len(movie_ids)):\n",
    "        if movie_ids[i] == i + 1 + skipped_integers:\n",
    "            ids_to_indices_arr = np.append(ids_to_indices_arr, i)\n",
    "        else:\n",
    "            while movie_ids[i] != i + 1 + skipped_integers:\n",
    "                ids_to_indices_arr = np.append(ids_to_indices_arr, 0)\n",
    "                skipped_integers += 1\n",
    "            ids_to_indices_arr = np.append(ids_to_indices_arr, i)\n",
    "    return ids_to_indices_arr\n",
    "\n",
    "\n",
    "def make_movie_indices(data_set, movie_ids):\n",
    "    \"\"\"\n",
    "    For a certain data set with movie ID's, return the corresponding indices of an array.\n",
    "    \n",
    "    data_set: Dataframe, dataframe containing ratings\n",
    "    movie_ids: array, array containing the ID's of all movies\n",
    "    \"\"\"\n",
    "    data_set_movie_ids = data_set['MovieID'].values - 1\n",
    "    ids_to_indices_arr = ids_to_indices(movie_ids)\n",
    "    movie_indices = ids_to_indices_arr[data_set_movie_ids]\n",
    "    return movie_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_naive_model(mean_rating, train_set, model_type, model_specific_params):\n",
    "    \"\"\"\n",
    "    Builds the naive model of a given type.\n",
    "    \n",
    "    mean_ratings: float, the mean of all ratings (in the train set)\n",
    "    train_set: Dataframe, dataframe containing the training data\n",
    "    model_type: string, type of the model (use '1', '2' or '3')\n",
    "    model_specific_parmas: dict, dictionary containing objects specific to the model type\n",
    "    \"\"\"\n",
    "    if model_type == '1':\n",
    "        def model(row):\n",
    "            return mean_rating\n",
    "        return model\n",
    "    elif model_type == '2':\n",
    "        mean_rating_per_user = np.array([])\n",
    "        \n",
    "        for user_id in model_specific_params['user_ids']:\n",
    "            user_subset = train_set[train_set['UserID'].values == user_id]\n",
    "            mean_rating_per_user = np.append(mean_rating_per_user, np.mean(user_subset['Rating']))\n",
    "        \n",
    "        # replace the user mean rating by the global mean rating if it is not available\n",
    "        mean_rating_per_user = np.where(np.isnan(mean_rating_per_user), mean_rating, mean_rating_per_user)\n",
    "\n",
    "        def model(row):\n",
    "            user_id = row['UserID'] - 1\n",
    "            return mean_rating_per_user[user_id]\n",
    "        return model\n",
    "    elif model_type == '3':\n",
    "        mean_rating_per_movie = np.array([])\n",
    "        \n",
    "        for movie_id in model_specific_params['movie_ids']:\n",
    "            movie_subset = train_set[train_set['MovieID'].values == movie_id]\n",
    "            mean_rating_per_movie = np.append(mean_rating_per_movie, np.mean(movie_subset['Rating']))\n",
    "        \n",
    "        # replace the movie mean rating by the global mean rating if it is not available\n",
    "        mean_rating_per_movie = np.where(np.isnan(mean_rating_per_movie), mean_rating, mean_rating_per_movie)\n",
    "\n",
    "        def model(row):\n",
    "            movie_id = row['MovieID'] - 1\n",
    "            return mean_rating_per_movie[model_specific_params['ids_to_indices_arr'][movie_id]]\n",
    "        return model\n",
    "    else:\n",
    "        return ValueError(f'Model type {model_type} is unknown.')\n",
    "\n",
    "    \n",
    "def naive_model_fold_error(ratings, train_indices, test_indices, model_type, model_specific_params):\n",
    "    \"\"\"\n",
    "    For a given fold, build the naive model and calculate its error on the test set.\n",
    "    \n",
    "    ratings: Dataframe, dataframe containing all ratings\n",
    "    train_indices: array, indices of the training set\n",
    "    test_indices array, indices of the test set\n",
    "    model_type: string, type of the model (use '1', '2' or '3')\n",
    "    model_specific_parmas: dict, dictionary containing objects specific to the model type\n",
    "    \"\"\"\n",
    "    train_set = ratings.iloc[train_indices]\n",
    "    test_set = ratings.iloc[test_indices]\n",
    "\n",
    "    mean_rating = train_set['Rating'].mean()\n",
    "\n",
    "    model = build_naive_model(mean_rating, train_set, model_type, model_specific_params)\n",
    "    \n",
    "    train_rmse, train_mae = rating_errors(train_set, model)\n",
    "    test_rmse, test_mae = rating_errors(test_set, model)\n",
    "    \n",
    "    mp_lock.acquire()\n",
    "    print('test rmse ', test_rmse, ' test mae ', test_mae)\n",
    "    mp_lock.release()\n",
    "    \n",
    "    return train_rmse, test_rmse, train_mae, test_mae\n",
    "\n",
    "\n",
    "def test_naive_model(data, model_type, n_folds=5):\n",
    "    \"\"\"\n",
    "    Calculates the error for a given model. Bias in training and test set selection is reduced\n",
    "    by using cross-validation. To speed up the process, we use multiprocessing to divide the \n",
    "    folds over different cores.\n",
    "    \n",
    "    data: tuple, contains the user, movie and rating dataframe\n",
    "    model_type: string, type of the model (use '1', '2' or '3')\n",
    "    n_folds: int, the number of folds to use for cross-validation\n",
    "    \"\"\"\n",
    "    users, movies, ratings = data\n",
    "    \n",
    "    cv = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n",
    "    \n",
    "    print(f'Testing naive model {model_type}...')\n",
    "    \n",
    "    if model_type == '1':\n",
    "        model_specific_params = {}\n",
    "    elif model_type == '2':\n",
    "        model_specific_params = {'user_ids': users['UserID']}\n",
    "    elif model_type == '3':\n",
    "        model_specific_params = {'movie_ids': movies['MovieID']}\n",
    "        model_specific_params.update({'ids_to_indices_arr': \n",
    "                                      ids_to_indices(model_specific_params['movie_ids'])})\n",
    "    else:\n",
    "        return ValueError(f'Model type {model_type} is unknown.')\n",
    "    \n",
    "    params = [(ratings,\n",
    "               train_indices, \n",
    "               test_indices, \n",
    "               model_type,\n",
    "               model_specific_params) for train_indices, test_indices in cv.split(ratings)]\n",
    "    pool = Pool(n_folds)\n",
    "    fold_errors = pool.starmap(naive_model_fold_error, params)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    train_rmse_per_fold = np.array([])\n",
    "    test_rmse_per_fold = np.array([])\n",
    "\n",
    "    train_mae_per_fold = np.array([])\n",
    "    test_mae_per_fold = np.array([])\n",
    "    \n",
    "    for errors in fold_errors:\n",
    "        train_rmse, test_rmse, train_mae, test_mae = errors\n",
    "        train_rmse_per_fold = np.append(train_rmse_per_fold, train_rmse)\n",
    "        test_rmse_per_fold = np.append(test_rmse_per_fold, test_rmse)\n",
    "        train_mae_per_fold = np.append(train_mae_per_fold, train_mae)\n",
    "        test_mae_per_fold = np.append(test_mae_per_fold, test_mae)\n",
    "    \n",
    "    print('mean train rmse', np.mean(train_rmse_per_fold))\n",
    "    print('mean test rmse', np.mean(test_rmse_per_fold))\n",
    "    print('mean train mae', np.mean(train_mae_per_fold))\n",
    "    print('mean test mae', np.mean(test_mae_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive model 1...\n",
      "test rmse  1.1149881214282826  test mae  0.9326130200574327\n",
      "test rmse  1.119729345062511  test mae  0.935986633921368\n",
      "test rmse  1.1162190943284103  test mae  0.9328028300165171\n",
      "test rmse  1.1180847521411108  test mae  0.9342782982805649\n",
      "test rmse  1.1164859396819586  test mae  0.9336288110062256\n",
      "mean train rmse 1.1171010587983956\n",
      "mean test rmse 1.1171014505284547\n",
      "mean train mae 0.9338605988479152\n",
      "mean test mae 0.9338619186564217\n",
      "Run time:  3.710469961166382  sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_naive_model(data, model_type='1')\n",
    "\n",
    "duration = time.time() - t0\n",
    "print('Run time: ', duration, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive model 2...\n",
      "test rmse  1.035874276898819  test mae  0.828891912964882\n",
      "test rmse  1.0394340299170062  test mae  0.8329949719978802\n",
      "test rmse  1.0347421440592997  test mae  0.8280632808182781\n",
      "test rmse  1.0331183095776237  test mae  0.8266677994054135\n",
      "test rmse  1.034231441705486  test mae  0.8281312090560236\n",
      "mean train rmse 1.0276727444564144\n",
      "mean test rmse 1.0354800404316467\n",
      "mean train mae 0.8227192335238153\n",
      "mean test mae 0.8289498348484955\n",
      "Run time:  13.072283744812012  sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_naive_model(data, model_type='2')\n",
    "\n",
    "duration = time.time() - t0\n",
    "print('Run time: ', duration, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing naive model 3...\n",
      "test rmse  0.9797988738672946  test mae  0.781917272264237\n",
      "test rmse  0.977362185832933  test mae  0.7799822708246255\n",
      "test rmse  0.9771504870073922  test mae  0.7811692659016145\n",
      "test rmse  0.9824065960588566  test mae  0.7851885462419244\n",
      "test rmse  0.9801152379579418  test mae  0.783163630987873\n",
      "mean train rmse 0.9742283446389879\n",
      "mean test rmse 0.9793666761448836\n",
      "mean train mae 0.7783363352007416\n",
      "mean test mae 0.7822841972440548\n",
      "Run time:  11.219701051712036  sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "test_naive_model(data, model_type='3')\n",
    "\n",
    "duration = time.time() - t0\n",
    "print('Run time: ', duration, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall mean rating \n",
    "mean_rating = ratings['Rating'].mean()\n",
    "\n",
    "# Lookup tables for naive models 2, 3, 4 and 5\n",
    "mean_rating_per_user = {user_id : ratings[ratings['UserID'] == user_id]['Rating'].mean() for user_id in users['UserID']}\n",
    "mean_rating_per_movie = {movie_id : ratings[ratings['MovieID'] == movie_id]['Rating'].mean() for movie_id in movies['MovieID']}\n",
    "\n",
    "def test_error_4_5(test_set,reg):\n",
    "    \n",
    "    mean_rating_per_movie_list = np.array([mean_rating_per_movie[movie_id] for movie_id in test_set['MovieID']])\n",
    "    mean_rating_per_user_list = np.array([mean_rating_per_user[user_id] for user_id in test_set['UserID']])\n",
    "    \n",
    "    ##the predicted rating value for the test set using model 4 and 5.\n",
    "    mean_rating_list = np.vstack((mean_rating_per_movie_list,mean_rating_per_user_list)).T\n",
    "    pre_rating = reg.predict(mean_rating_list)[0]\n",
    "    \n",
    "    rating_error = ((test_set['Rating'] - pre_rating)**2).mean()**(1/2)\n",
    "    \n",
    "    return rating_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_model_4_5(data, subset:int=None):\n",
    "    users, movies, ratings = data\n",
    "    \n",
    "    cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    \n",
    "    rating_errors_4 = np.array([])\n",
    "    rating_errors_5 = np.array([])\n",
    "    \n",
    "    for train_index, test_index in cv.split(ratings):\n",
    "        train_set = ratings.iloc[train_index]\n",
    "        test_set = ratings.iloc[test_index]\n",
    "        user_ids = train_set['UserID']\n",
    "        \n",
    "        mean_rating_per_movie = {movie_id : train_set[train_set['MovieID'] == movie_id]['Rating'].mean() for movie_id in movies['MovieID']}\n",
    "        mean_rating_per_user = {user_id : train_set[train_set['UserID'] == user_id]['Rating'].mean() for user_id in users['UserID']}\n",
    "        \n",
    "        \n",
    "        ##the lists of mean Ritem and mean Ruser for each rating in the train_set\n",
    "        mean_rating_per_movie_list = np.array([mean_rating_per_movie[movie_id] for movie_id in train_set['MovieID']])\n",
    "        mean_rating_per_user_list = np.array([mean_rating_per_user[user_id] for user_id in train_set['UserID']])\n",
    "        \n",
    "        ## stack the Ritem and Ruser lists for linear fitting\n",
    "        mean_rating_list = np.vstack((mean_rating_per_movie_list, mean_rating_per_user_list)).T\n",
    "        \n",
    "        ## uisng Ordinary least squares Linear Regression to find alpha beta and gamma\n",
    "        reg_4 = LinearRegression(fit_intercept=False).fit(mean_rating_list, train_set['Rating'])\n",
    "        reg_5 = LinearRegression(fit_intercept=True).fit(mean_rating_list, train_set['Rating'])\n",
    "\n",
    "        \n",
    "        rating_err_4 = test_error_4_5(test_set,reg_4)\n",
    "        rating_err_5 = test_error_4_5(test_set,reg_5)\n",
    "        \n",
    "        rating_errors_4 = np.append(rating_errors_4,rating_err_4)\n",
    "        rating_errors_5 = np.append(rating_errors_5,rating_err_5)\n",
    "        \n",
    "        print('Rating Error for Naive Model 4:', rating_err_4)\n",
    "        print('Rating Error for Naive Model 5:', rating_err_5)\n",
    "    \n",
    "    print('Mean Rating Error for Naive Model 4:', np.mean(rating_errors_4))   \n",
    "    print('Mean Rating Error for Naive Model 5:', np.mean(rating_errors_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Error for Naive Model 4: 1.355883535175115\n",
      "Rating Error for Naive Model 5: 1.6272636448876663\n",
      "Rating Error for Naive Model 4: 1.258558362688454\n",
      "Rating Error for Naive Model 5: 1.455643650961396\n",
      "Rating Error for Naive Model 4: 1.276126365946629\n",
      "Rating Error for Naive Model 5: 1.4850790659505142\n",
      "Rating Error for Naive Model 4: 1.131717217519981\n",
      "Rating Error for Naive Model 5: 1.1786016924171567\n",
      "Rating Error for Naive Model 4: 1.1960107991397175\n",
      "Rating Error for Naive Model 5: 1.329055713915798\n",
      "Mean Rating Error for Naive Model 4: 1.2436592560939794\n",
      "Mean Rating Error for Naive Model 5: 1.4151287536265063\n"
     ]
    }
   ],
   "source": [
    "test_naive_model_4_5(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing matrix factorization model...\n",
      "step  0  rmse train  2.6014349134883816  rmse test  2.6108482677981546\n",
      "step  0  rmse train  2.6034737677922672  rmse test  2.602706385409006\n",
      "step  0  rmse train  2.604653921123481  rmse test  2.597979022703702\n",
      "step  0  rmse train  2.603880273225993  rmse test  2.6010792506915794\n",
      "step  0  rmse train  2.603157576718127  rmse test  2.603971140974104\n",
      "step  1  rmse train  1.5012680199728958  rmse test  1.534151677834486\n",
      "step  1  rmse train  1.490806497728355  rmse test  1.5275541256802776\n",
      "step  1  rmse train  1.5101981483222924  rmse test  1.5464018564118935\n",
      "step  1  rmse train  1.4934905479371927  rmse test  1.5334731026744834\n",
      "step  1  rmse train  1.5033886957015101  rmse test  1.53502091138808\n",
      "step  2  rmse train  0.9876743967178091  rmse test  1.042040875246315\n",
      "step  2  rmse train  0.9846294981070677  rmse test  1.0471146012033656\n",
      "step  2  rmse train  0.9857446607027487  rmse test  1.0456966656584832\n",
      "step  2  rmse train  0.9896938375219059  rmse test  1.045363537206324\n",
      "step  2  rmse train  0.9885937433662817  rmse test  1.0449475068615721\n",
      "step  3  rmse train  0.9327542131164314  rmse test  0.9886969225187661\n",
      "step  3  rmse train  0.9309396495353187  rmse test  0.9950234273230054\n",
      "step  3  rmse train  0.9323712167571321  rmse test  0.9896408627673906\n",
      "step  3  rmse train  0.9317301059618586  rmse test  0.9917378115463894\n",
      "step  3  rmse train  0.9325643452429582  rmse test  0.9898894796416655\n",
      "step  4  rmse train  0.916663347825531  rmse test  0.9732838689199721\n",
      "step  4  rmse train  0.9160358818887199  rmse test  0.9742413618626015\n",
      "step  4  rmse train  0.9149239379597128  rmse test  0.9797094446703418\n",
      "step  4  rmse train  0.915713042810964  rmse test  0.9761112534966365\n",
      "step  4  rmse train  0.9164423513732142  rmse test  0.9739614583226871\n",
      "step  5  rmse train  0.9073523491949141  rmse test  0.9647729463276256\n",
      "step  5  rmse train  0.9064040752571075  rmse test  0.9673508035480372\n",
      "step  5  rmse train  0.9056545407394729  rmse test  0.9711193212482861\n",
      "step  5  rmse train  0.9066860502720457  rmse test  0.9657443530144323\n",
      "step  5  rmse train  0.9071629266204202  rmse test  0.9650983050503231\n",
      "step  6  rmse train  0.9000731290748096  rmse test  0.9584207237065887\n",
      "step  6  rmse train  0.8994289544436391  rmse test  0.9594722970820472\n",
      "step  6  rmse train  0.8991357450149612  rmse test  0.9608031806098124\n",
      "step  6  rmse train  0.8984204527971537  rmse test  0.9646602224224218\n",
      "step  6  rmse train  0.8999239935821651  rmse test  0.958510198963063\n",
      "step  7  rmse train  0.8936323499819967  rmse test  0.9530923227173203\n",
      "step  7  rmse train  0.893072802381895  rmse test  0.9541716507502654\n",
      "step  7  rmse train  0.8927496892149592  rmse test  0.9552953564495507\n",
      "step  7  rmse train  0.8920755500227195  rmse test  0.9592279565563195\n",
      "step  7  rmse train  0.893517933816505  rmse test  0.95297069323178\n",
      "step  8  rmse train  0.887696022836906  rmse test  0.9483751166204398\n",
      "step  8  rmse train  0.8872301118761563  rmse test  0.9494433038745616\n",
      "step  8  rmse train  0.8869085139695119  rmse test  0.950452744898002\n",
      "step  8  rmse train  0.8862716444284556  rmse test  0.9544443840701174\n",
      "step  8  rmse train  0.8875717813797122  rmse test  0.9480369506469314\n",
      "step  9  rmse train  0.8822274757927295  rmse test  0.9441781737472837\n",
      "step  9  rmse train  0.8818393281448219  rmse test  0.9451916872193438\n",
      "step  9  rmse train  0.8815544723556027  rmse test  0.9461608616339876\n",
      "step  9  rmse train  0.8809390589105888  rmse test  0.9501761613205741\n",
      "step  9  rmse train  0.8820642561578813  rmse test  0.9436203359973304\n",
      "step  10  rmse train  0.877224626717209  rmse test  0.9404612200636431\n",
      "step  10  rmse train  0.8768908597212339  rmse test  0.9413904904143754\n",
      "step  10  rmse train  0.8766619102225276  rmse test  0.9423742772398752\n",
      "step  10  rmse train  0.8760503971626109  rmse test  0.9463945124183933\n",
      "step  10  rmse train  0.877004239145163  rmse test  0.9396962289140521\n",
      "step  11  rmse train  0.8726401925535797  rmse test  0.9371730715379621\n",
      "step  11  rmse train  0.8723423049994222  rmse test  0.938002725273743\n",
      "step  11  rmse train  0.8721778321012974  rmse test  0.9390199051433737\n",
      "step  11  rmse train  0.8715542450435655  rmse test  0.9430472558534535\n",
      "step  11  rmse train  0.872355650882307  rmse test  0.9362243420605925\n",
      "step  12  rmse train  0.8684034133349553  rmse test  0.9342482689360299\n",
      "step  12  rmse train  0.8681251993969226  rmse test  0.9349645207186665\n",
      "step  12  rmse train  0.8680275949596354  rmse test  0.9360257247041609\n",
      "step  12  rmse train  0.8673813705745488  rmse test  0.9400616581523439\n",
      "step  12  rmse train  0.8680531908010635  rmse test  0.9331356784521337\n",
      "step  13  rmse train  0.864439436040615  rmse test  0.9316068488167494\n",
      "step  13  rmse train  0.8641678312041798  rmse test  0.9322109712177553\n",
      "step  13  rmse train  0.8641399101301468  rmse test  0.9333256321015115\n",
      "step  13  rmse train  0.8634670138202049  rmse test  0.9373620876508217\n",
      "step  13  rmse train  0.8640264010162629  rmse test  0.9303489232439299\n",
      "step  14  rmse train  0.8606866951228122  rmse test  0.9291850852617173\n",
      "step  14  rmse train  0.8604123120436191  rmse test  0.9296673021126711\n",
      "step  14  rmse train  0.8604550939329261  rmse test  0.9308510841183484\n",
      "step  14  rmse train  0.8597585155516843  rmse test  0.9348846507670229\n",
      "step  14  rmse train  0.8602227808313113  rmse test  0.9277996504572901\n",
      "step  15  rmse train  0.8571004300907741  rmse test  0.9269353881964755\n",
      "step  15  rmse train  0.856817490187159  rmse test  0.9272855819922371\n",
      "step  15  rmse train  0.8569305962466494  rmse test  0.928551725460486\n",
      "step  15  rmse train  0.8562217250467629  rmse test  0.9325824934894414\n",
      "step  15  rmse train  0.8566002608120804  rmse test  0.925437757638741\n",
      "step  16  rmse train  0.8536474215178151  rmse test  0.9248176779803731\n",
      "step  16  rmse train  0.8533586090200714  rmse test  0.9250515888753927\n",
      "step  16  rmse train  0.8528339252271285  rmse test  0.9304181811559802\n",
      "step  16  rmse train  0.8535379438972657  rmse test  0.9263902884605721\n",
      "step  16  rmse train  0.8531327593064547  rmse test  0.9232355088223704\n",
      "step  17  rmse train  0.8503124588516899  rmse test  0.9228088218925729\n",
      "step  17  rmse train  0.8500220640709865  rmse test  0.922942250327269\n",
      "step  17  rmse train  0.8495820175649416  rmse test  0.9283636780529293\n",
      "step  17  rmse train  0.850262676417384  rmse test  0.9243388977624659\n",
      "step  17  rmse train  0.8498033598120572  rmse test  0.921164255119394\n",
      "step  18  rmse train  0.8470906108714554  rmse test  0.9209004757061889\n",
      "step  18  rmse train  0.846803624734046  rmse test  0.9209421601659611\n",
      "step  18  rmse train  0.8464596544062692  rmse test  0.9264128725126751\n",
      "step  18  rmse train  0.8466024757985412  rmse test  0.9192066064161367\n",
      "step  18  rmse train  0.8470998678068418  rmse test  0.9223830124785328\n",
      "step  19  rmse train  0.8439823190953523  rmse test  0.9190863632832876\n",
      "step  19  rmse train  0.8437029780369604  rmse test  0.9190434908957765\n",
      "step  19  rmse train  0.8434645135390066  rmse test  0.9245503677962862\n",
      "step  19  rmse train  0.8435249217234928  rmse test  0.9173500825515635\n",
      "step  19  rmse train  0.8440498398545436  rmse test  0.9205104810670219\n",
      "step  20  rmse train  0.840991346521725  rmse test  0.9173612681357806\n",
      "step  20  rmse train  0.8407227843391635  rmse test  0.9172440550763112\n",
      "step  20  rmse train  0.8405954510338287  rmse test  0.9227740869789198\n",
      "step  20  rmse train  0.8405694376610491  rmse test  0.9155862053271967\n",
      "step  20  rmse train  0.8411151173325544  rmse test  0.9187228898147667\n",
      "step  21  rmse train  0.8381208988561988  rmse test  0.9157236961388049\n",
      "step  21  rmse train  0.8378650012787051  rmse test  0.9155358198122612\n",
      "step  21  rmse train  0.8378506885047823  rmse test  0.9210828527524352\n",
      "step  21  rmse train  0.8377349016546706  rmse test  0.913912957784989\n",
      "step  21  rmse train  0.8382971317689539  rmse test  0.917021119941728\n",
      "step  22  rmse train  0.835130233370657  rmse test  0.913921278829252\n",
      "step  22  rmse train  0.8353728285546889  rmse test  0.9141713733386783\n",
      "step  22  rmse train  0.8352275177277138  rmse test  0.9194738136955001\n",
      "step  22  rmse train  0.8350197940428211  rmse test  0.9123292069199712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  22  rmse train  0.8355954094111641  rmse test  0.9154030669379927\n",
      "step  23  rmse train  0.8325168566900778  rmse test  0.912393592754138\n",
      "step  23  rmse train  0.8327474070505134  rmse test  0.9127044139521816\n",
      "step  23  rmse train  0.8327228355248704  rmse test  0.917943942356999\n",
      "step  23  rmse train  0.8324223196613634  rmse test  0.9108287411773878\n",
      "step  23  rmse train  0.8330094992265126  rmse test  0.9138671363741249\n",
      "step  24  rmse train  0.8300221707337536  rmse test  0.9109500533830788\n",
      "step  24  rmse train  0.8302435884699132  rmse test  0.9113147756711542\n",
      "step  24  rmse train  0.8299404141522921  rmse test  0.9094035241188145\n",
      "step  24  rmse train  0.8303327641652104  rmse test  0.9164921616682399\n",
      "step  24  rmse train  0.8305365879107625  rmse test  0.9124126506281484\n",
      "step  25  rmse train  0.8276425439448859  rmse test  0.9095876403327731\n",
      "step  25  rmse train  0.8278582095802374  rmse test  0.910000059165378\n",
      "step  25  rmse train  0.8280526341953256  rmse test  0.9151165863108488\n",
      "step  25  rmse train  0.8275703554624867  rmse test  0.9080557263431143\n",
      "step  25  rmse train  0.8281730326872998  rmse test  0.911032805415106\n",
      "step  26  rmse train  0.8253733160272914  rmse test  0.908298164071143\n",
      "step  26  rmse train  0.8255869745860167  rmse test  0.9087581718823928\n",
      "step  26  rmse train  0.8258771074257178  rmse test  0.9138120084352731\n",
      "step  26  rmse train  0.8253079236579118  rmse test  0.9067809418871619\n",
      "step  26  rmse train  0.8259135240390774  rmse test  0.9097245327550482\n",
      "step  27  rmse train  0.8232098845451394  rmse test  0.9070798819682963\n",
      "step  27  rmse train  0.8234253715370667  rmse test  0.9075812777437552\n",
      "step  27  rmse train  0.8238005301673726  rmse test  0.9125744786484495\n",
      "step  27  rmse train  0.8231490524413938  rmse test  0.9055746366677873\n",
      "step  27  rmse train  0.8237542233194797  rmse test  0.9084855675228156\n",
      "step  28  rmse train  0.8211469175603041  rmse test  0.9059288597606454\n",
      "step  28  rmse train  0.8213685736956488  rmse test  0.9064689672457686\n",
      "step  28  rmse train  0.8218179889157403  rmse test  0.9114001231880035\n",
      "step  28  rmse train  0.8210891658476498  rmse test  0.9044338044698889\n",
      "step  28  rmse train  0.8216903089002338  rmse test  0.9073130675954669\n",
      "step  29  rmse train  0.8191796557941713  rmse test  0.9048441695772137\n",
      "step  29  rmse train  0.8194110047427627  rmse test  0.9054170931218616\n",
      "step  29  rmse train  0.8199245237761765  rmse test  0.9102874067361442\n",
      "step  29  rmse train  0.8191236710630437  rmse test  0.9033557440672116\n",
      "step  29  rmse train  0.8197169816440233  rmse test  0.90620321054609\n",
      "step  30  rmse train  0.8173027790836254  rmse test  0.9038162244333673\n",
      "step  30  rmse train  0.8175474074114035  rmse test  0.9044226151263909\n",
      "step  30  rmse train  0.8181149158287815  rmse test  0.9092336277125964\n",
      "step  30  rmse train  0.8172474414073636  rmse test  0.9023356278096307\n",
      "step  30  rmse train  0.8178297548952945  rmse test  0.9051494259278692\n",
      "step  31  rmse train  0.8155112464985659  rmse test  0.9028444101995697\n",
      "step  31  rmse train  0.8157722368124402  rmse test  0.9034809981846026\n",
      "step  31  rmse train  0.8163848324385273  rmse test  0.9082335098655422\n",
      "step  31  rmse train  0.8154558025920251  rmse test  0.901366687794583\n",
      "step  31  rmse train  0.8160241198880954  rmse test  0.9041523738459039\n",
      "step  32  rmse train  0.8138000816277428  rmse test  0.9019239037466884\n",
      "step  32  rmse train  0.8140806308861304  rmse test  0.9025878680840791\n",
      "step  32  rmse train  0.8147300090278412  rmse test  0.9072838507909816\n",
      "step  32  rmse train  0.8137439031408704  rmse test  0.9004486140713034\n",
      "step  32  rmse train  0.8142966441534403  rmse test  0.9032072493661949\n",
      "step  33  rmse train  0.8121650944655396  rmse test  0.9010494086629246\n",
      "step  33  rmse train  0.8124680965009582  rmse test  0.9017395625865664\n",
      "step  33  rmse train  0.8131462710536278  rmse test  0.9063820456378148\n",
      "step  33  rmse train  0.8121074578261214  rmse test  0.8995763475351519\n",
      "step  33  rmse train  0.8126436303846929  rmse test  0.9023127006903756\n",
      "step  34  rmse train  0.8106018250043239  rmse test  0.9002198026139325\n",
      "step  34  rmse train  0.8109301627868523  rmse test  0.9009338756396629\n",
      "step  34  rmse train  0.8116296342636077  rmse test  0.9055261575238297\n",
      "step  34  rmse train  0.810542276184135  rmse test  0.8987475496893481\n",
      "step  34  rmse train  0.8110612371134821  rmse test  0.9014652840781752\n",
      "step  35  rmse train  0.8091062903831772  rmse test  0.8994328569184253\n",
      "step  35  rmse train  0.809462452555899  rmse test  0.9001664641577711\n",
      "step  35  rmse train  0.8101763524458698  rmse test  0.9047127489741686\n",
      "step  35  rmse train  0.809044440928283  rmse test  0.8979598415978045\n",
      "step  35  rmse train  0.8095461175071778  rmse test  0.9006626685158287\n",
      "step  36  rmse train  0.8076747023365739  rmse test  0.898683134104644\n",
      "step  36  rmse train  0.8080607916922734  rmse test  0.8994354984345149\n",
      "step  36  rmse train  0.80878291280069  rmse test  0.903934842465439\n",
      "step  36  rmse train  0.8076101621251524  rmse test  0.8972111918695577\n",
      "step  36  rmse train  0.8080952600347143  rmse test  0.8999003458352846\n",
      "step  37  rmse train  0.8063032322427625  rmse test  0.8979679404636846\n",
      "step  37  rmse train  0.8067215024762999  rmse test  0.8987396416732097\n",
      "step  37  rmse train  0.8074458909415856  rmse test  0.9031929644132928\n",
      "step  37  rmse train  0.8062360739197447  rmse test  0.8964982523571285\n",
      "step  37  rmse train  0.8067058733844128  rmse test  0.8991766972749925\n",
      "step  38  rmse train  0.8049887013900321  rmse test  0.897285535281\n",
      "step  38  rmse train  0.805441098445839  rmse test  0.8980760252195646\n",
      "step  38  rmse train  0.8061622480684044  rmse test  0.9024856923819231\n",
      "step  38  rmse train  0.8049189684680295  rmse test  0.8958185498454385\n",
      "step  38  rmse train  0.8053749601974146  rmse test  0.8984898435479305\n",
      "step  39  rmse train  0.8037281446058767  rmse test  0.8966333276305666\n",
      "step  39  rmse train  0.8042162889231849  rmse test  0.8974435641502108\n",
      "step  39  rmse train  0.8049290389807078  rmse test  0.9018094972078631\n",
      "step  39  rmse train  0.8040993932040135  rmse test  0.8978384401580556\n",
      "step  39  rmse train  0.803655818799006  rmse test  0.8951715975277135\n",
      "step  40  rmse train  0.8025187332247169  rmse test  0.8960106567709676\n",
      "step  40  rmse train  0.803044074403694  rmse test  0.8968386762933648\n",
      "step  40  rmse train  0.8037437094391734  rmse test  0.9011609107780498\n",
      "step  40  rmse train  0.8028768218713436  rmse test  0.8972188189098167\n",
      "step  40  rmse train  0.8024437223545071  rmse test  0.894554992661655\n",
      "step  41  rmse train  0.8013578260739759  rmse test  0.8954148316064244\n",
      "step  41  rmse train  0.8019215854304748  rmse test  0.8962623932663345\n",
      "step  41  rmse train  0.8026036848588564  rmse test  0.9005372188522902\n",
      "step  41  rmse train  0.8012801781037522  rmse test  0.893966227356128\n",
      "step  41  rmse train  0.8017046985941034  rmse test  0.8966298194606114\n",
      "step  42  rmse train  0.8002429047509553  rmse test  0.8948455675138027\n",
      "step  42  rmse train  0.8008461312011897  rmse test  0.8957130844768125\n",
      "step  42  rmse train  0.8015068016430327  rmse test  0.8999377383496557\n",
      "step  42  rmse train  0.8001628928315995  rmse test  0.893404947517484\n",
      "step  42  rmse train  0.8005805744998248  rmse test  0.8960711979238267\n",
      "step  43  rmse train  0.799171902060568  rmse test  0.8943005354197213\n",
      "step  43  rmse train  0.7998152751308196  rmse test  0.8951896646620163\n",
      "step  43  rmse train  0.8004509136104574  rmse test  0.8993627076904724\n",
      "step  43  rmse train  0.7990896586723341  rmse test  0.892869533959874\n",
      "step  43  rmse train  0.7995022168184933  rmse test  0.8955410586257694\n",
      "step  44  rmse train  0.7981426836525973  rmse test  0.8937761575057973\n",
      "step  44  rmse train  0.79882671924192  rmse test  0.8946909992980122\n",
      "step  44  rmse train  0.7994339986266281  rmse test  0.8988095786379273\n",
      "step  44  rmse train  0.7980584346945048  rmse test  0.8923595661144553\n",
      "step  44  rmse train  0.7984674499651324  rmse test  0.895037248200015\n",
      "step  45  rmse train  0.7971532012348942  rmse test  0.8932714171755922\n",
      "step  45  rmse train  0.7978782532758103  rmse test  0.8942148101248929\n",
      "step  45  rmse train  0.7984542082562904  rmse test  0.8982759061293734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  45  rmse train  0.7970672387822092  rmse test  0.8918730722923254\n",
      "step  45  rmse train  0.797474120793486  rmse test  0.8945567253317781\n",
      "step  46  rmse train  0.7962017037571956  rmse test  0.8927844053426663\n",
      "step  46  rmse train  0.796967955242233  rmse test  0.8937608487082237\n",
      "step  46  rmse train  0.7975098370367227  rmse test  0.8977619580360319\n",
      "step  46  rmse train  0.7961142634132302  rmse test  0.8914083539818868\n",
      "step  46  rmse train  0.7965202572489513  rmse test  0.8940987376521311\n",
      "step  47  rmse train  0.7952865105671972  rmse test  0.8923154169329278\n",
      "step  47  rmse train  0.7960937521843375  rmse test  0.8933271020062161\n",
      "step  47  rmse train  0.7965992312884598  rmse test  0.8972641767440732\n",
      "step  47  rmse train  0.7951977428954843  rmse test  0.8909623901595342\n",
      "step  47  rmse train  0.7956038521700773  rmse test  0.8936620604883856\n",
      "step  48  rmse train  0.7944060714617949  rmse test  0.8918648018960458\n",
      "step  48  rmse train  0.7952538879244454  rmse test  0.8929130640440348\n",
      "step  48  rmse train  0.7957208028603183  rmse test  0.8967826028688615\n",
      "step  48  rmse train  0.7943161897655274  rmse test  0.8905366001435866\n",
      "step  48  rmse train  0.7947230617684543  rmse test  0.8932446393899627\n",
      "step  49  rmse train  0.7935587859297719  rmse test  0.8914320095914852\n",
      "step  49  rmse train  0.79444672902602  rmse test  0.8925176600928028\n",
      "step  49  rmse train  0.7948731256595287  rmse test  0.8963183295309427\n",
      "step  49  rmse train  0.7934680269631244  rmse test  0.8901287446164434\n",
      "step  49  rmse train  0.7938761862791244  rmse test  0.8928447615439578\n",
      "step  50  rmse train  0.7927431559484301  rmse test  0.8910161398039568\n",
      "step  50  rmse train  0.7936706703493083  rmse test  0.8921382602214385\n",
      "step  50  rmse train  0.7940548968632  rmse test  0.8958697942876206\n",
      "step  50  rmse train  0.7926517646293254  rmse test  0.8897375360692676\n",
      "step  50  rmse train  0.7930615081782156  rmse test  0.8924625452658363\n",
      "step  51  rmse train  0.7919579322920225  rmse test  0.8906154137891968\n",
      "step  51  rmse train  0.792924207377545  rmse test  0.891775163492348\n",
      "step  51  rmse train  0.7932649331516346  rmse test  0.8954360112107471\n",
      "step  51  rmse train  0.7918660411428746  rmse test  0.8893618516988782\n",
      "step  51  rmse train  0.792277552594969  rmse test  0.892096320212115\n",
      "step  52  rmse train  0.7912017507310406  rmse test  0.8902288177109885\n",
      "step  52  rmse train  0.7922059501893383  rmse test  0.8914278773872265\n",
      "step  52  rmse train  0.7925020888600618  rmse test  0.8950153793676767\n",
      "step  52  rmse train  0.7911095689905766  rmse test  0.8890014273204092\n",
      "step  52  rmse train  0.7915227697356519  rmse test  0.8917449197885831\n",
      "step  53  rmse train  0.7904735171581467  rmse test  0.8898564329100841\n",
      "step  53  rmse train  0.7915144684968242  rmse test  0.8910949254904579\n",
      "step  53  rmse train  0.7917652487475073  rmse test  0.8946090167404627\n",
      "step  53  rmse train  0.7903810476591162  rmse test  0.8886553200490629\n",
      "step  53  rmse train  0.790795849378866  rmse test  0.8914077560823392\n",
      "step  54  rmse train  0.7897720827358425  rmse test  0.8894974812898057\n",
      "step  54  rmse train  0.7908486045763792  rmse test  0.8907759699578729\n",
      "step  54  rmse train  0.7910531959205165  rmse test  0.8942148549612007\n",
      "step  54  rmse train  0.7896793771668227  rmse test  0.8883222089729278\n",
      "step  54  rmse train  0.7900954136498958  rmse test  0.8910836457704608\n",
      "step  55  rmse train  0.7890963247822977  rmse test  0.8891513105037462\n",
      "step  55  rmse train  0.7902071596706184  rmse test  0.8904704038548293\n",
      "step  55  rmse train  0.7903650643007963  rmse test  0.893833218942979\n",
      "step  55  rmse train  0.7890034221475104  rmse test  0.8880013034619809\n",
      "step  55  rmse train  0.7894202067815528  rmse test  0.8907721451892725\n",
      "step  56  rmse train  0.7884452126095322  rmse test  0.8888178950116717\n",
      "step  56  rmse train  0.7895889608555926  rmse test  0.8901781563453706\n",
      "step  56  rmse train  0.7896998588812313  rmse test  0.8934632283612085\n",
      "step  56  rmse train  0.7883520958452355  rmse test  0.8876930918437098\n",
      "step  56  rmse train  0.7887690398958843  rmse test  0.8904721501815156\n",
      "step  57  rmse train  0.7878177098614882  rmse test  0.8884966090844868\n",
      "step  57  rmse train  0.788992956783215  rmse test  0.8898980829475236\n",
      "step  57  rmse train  0.7890566828363237  rmse test  0.8931026908359923\n",
      "step  57  rmse train  0.7877243167428651  rmse test  0.8873973070131409\n",
      "step  57  rmse train  0.7881407862276711  rmse test  0.890182966174424\n",
      "step  58  rmse train  0.7872127550368241  rmse test  0.88818682190568\n",
      "step  58  rmse train  0.7884181388423588  rmse test  0.8896299199740814\n",
      "step  58  rmse train  0.788434731056129  rmse test  0.892752232541172\n",
      "step  58  rmse train  0.7871191566260025  rmse test  0.8871138711509163\n",
      "step  58  rmse train  0.7875343804593037  rmse test  0.8899031161367467\n",
      "step  59  rmse train  0.7866294458086721  rmse test  0.8878873773381932\n",
      "step  59  rmse train  0.7878635215204907  rmse test  0.8893729754889681\n",
      "step  59  rmse train  0.787833176496389  rmse test  0.8924119906092708\n",
      "step  59  rmse train  0.7865356458530857  rmse test  0.886841323090201\n",
      "step  59  rmse train  0.7869488661322966  rmse test  0.8896331128635633\n",
      "step  60  rmse train  0.7860669058179478  rmse test  0.887597809408184\n",
      "step  60  rmse train  0.7873281793230411  rmse test  0.8891272082667051\n",
      "step  60  rmse train  0.7872512412753635  rmse test  0.8920815855679631\n",
      "step  60  rmse train  0.7859728911530021  rmse test  0.886579477124342\n",
      "step  60  rmse train  0.7863832994250332  rmse test  0.8893718840157975\n",
      "step  61  rmse train  0.785524152357487  rmse test  0.887318080710394\n",
      "step  61  rmse train  0.7868112616630094  rmse test  0.8888909025057071\n",
      "step  61  rmse train  0.7866881415896513  rmse test  0.8917614701812492\n",
      "step  61  rmse train  0.7854299496470231  rmse test  0.8863273782839975\n",
      "step  61  rmse train  0.7858367458947418  rmse test  0.8891187209229288\n",
      "step  62  rmse train  0.7850004341493881  rmse test  0.887047660729372\n",
      "step  62  rmse train  0.7863119348221346  rmse test  0.8886643767118955\n",
      "step  62  rmse train  0.7861432538913096  rmse test  0.8914511568043731\n",
      "step  62  rmse train  0.7849060109682706  rmse test  0.8860845810688494\n",
      "step  62  rmse train  0.7853083427549334  rmse test  0.8888745947867912\n",
      "step  63  rmse train  0.7844949903815098  rmse test  0.8867862041539581\n",
      "step  63  rmse train  0.7858293459785485  rmse test  0.8884472008896863\n",
      "step  63  rmse train  0.7856158911923266  rmse test  0.8911501563971638\n",
      "step  63  rmse train  0.7844002011973231  rmse test  0.8858506137924078\n",
      "step  63  rmse train  0.7847973032548984  rmse test  0.8886387644531657\n",
      "step  64  rmse train  0.7840070020076179  rmse test  0.886533055212178\n",
      "step  64  rmse train  0.7853627646499863  rmse test  0.8882392109156807\n",
      "step  64  rmse train  0.7851054323055213  rmse test  0.8908586559686777\n",
      "step  64  rmse train  0.7839118337179303  rmse test  0.8856250860945698\n",
      "step  64  rmse train  0.7843028503436602  rmse test  0.888410867932021\n",
      "step  65  rmse train  0.7835357262317756  rmse test  0.8862884132395298\n",
      "step  65  rmse train  0.7849115421810972  rmse test  0.8880399383090339\n",
      "step  65  rmse train  0.7846112126820964  rmse test  0.8905767045911591\n",
      "step  65  rmse train  0.7834401999348124  rmse test  0.8854073566218569\n",
      "step  65  rmse train  0.7838241965056898  rmse test  0.8881905654950731\n",
      "step  66  rmse train  0.7830804646472559  rmse test  0.8860519009937636\n",
      "step  66  rmse train  0.78447499296437  rmse test  0.8878491895078028\n",
      "step  66  rmse train  0.7841326770733804  rmse test  0.8903033142878362\n",
      "step  66  rmse train  0.7829845865593609  rmse test  0.885197851843239\n",
      "step  66  rmse train  0.7833607223332444  rmse test  0.8879769799035269\n",
      "step  67  rmse train  0.7826405159056414  rmse test  0.8858232939087396\n",
      "step  67  rmse train  0.7840525035973525  rmse test  0.8876664132523867\n",
      "step  67  rmse train  0.7836692983175128  rmse test  0.8900382860978735\n",
      "step  67  rmse train  0.7825443333211336  rmse test  0.8849961212391333\n",
      "step  67  rmse train  0.7829117738281121  rmse test  0.8877702168748385\n",
      "step  68  rmse train  0.7822152578788684  rmse test  0.8856021746565971\n",
      "step  68  rmse train  0.7836434784008874  rmse test  0.887490704101942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  68  rmse train  0.7832205514525419  rmse test  0.8897811997555324\n",
      "step  68  rmse train  0.7821187859939728  rmse test  0.8848014468719387\n",
      "step  68  rmse train  0.7824767870867165  rmse test  0.8875700920315632\n",
      "step  69  rmse train  0.7818040461399502  rmse test  0.8853881181384906\n",
      "step  69  rmse train  0.7832473442643687  rmse test  0.8873211368500226\n",
      "step  69  rmse train  0.7827859188057256  rmse test  0.8895321575763997\n",
      "step  69  rmse train  0.7817072722270665  rmse test  0.8846137186757783\n",
      "step  69  rmse train  0.7820551625860693  rmse test  0.8873746614503563\n",
      "step  70  rmse train  0.7814063120342163  rmse test  0.885180633575273\n",
      "step  70  rmse train  0.7823649118795997  rmse test  0.8892902412630449\n",
      "step  70  rmse train  0.7828635622981213  rmse test  0.8871578698152691\n",
      "step  70  rmse train  0.7813092344357805  rmse test  0.8844322804427426\n",
      "step  70  rmse train  0.7816463385005938  rmse test  0.8871841879871267\n",
      "step  71  rmse train  0.781021476092387  rmse test  0.8849793867108188\n",
      "step  71  rmse train  0.7819570423828822  rmse test  0.8890557579088865\n",
      "step  71  rmse train  0.7824916253687837  rmse test  0.8870009259645333\n",
      "step  71  rmse train  0.7809241727011942  rmse test  0.884256688238189\n",
      "step  71  rmse train  0.7812498401733742  rmse test  0.8869980422593782\n",
      "step  72  rmse train  0.7806489473626258  rmse test  0.8847845403823928\n",
      "step  72  rmse train  0.7815618479674306  rmse test  0.8888289803250203\n",
      "step  72  rmse train  0.7821310616160287  rmse test  0.8868494403964505\n",
      "step  72  rmse train  0.7805515519398488  rmse test  0.8840867391126678\n",
      "step  72  rmse train  0.7808651735082558  rmse test  0.8868172214364283\n",
      "step  73  rmse train  0.7802881764058778  rmse test  0.8845955755362469\n",
      "step  73  rmse train  0.7811788905841867  rmse test  0.8886092937694483\n",
      "step  73  rmse train  0.7817813948334786  rmse test  0.88670277062891\n",
      "step  73  rmse train  0.7801908675387738  rmse test  0.8839222559603663\n",
      "step  73  rmse train  0.7804918598340023  rmse test  0.8866415646852528\n",
      "step  74  rmse train  0.7799387110929574  rmse test  0.8844126352119347\n",
      "step  74  rmse train  0.7808077484049986  rmse test  0.8883958901750386\n",
      "step  74  rmse train  0.7814421850574758  rmse test  0.8865608258009205\n",
      "step  74  rmse train  0.7798416081470564  rmse test  0.8837632250562217\n",
      "step  74  rmse train  0.7801294708261729  rmse test  0.8864707222764093\n",
      "Run time:  851.4256012439728  sec\n"
     ]
    }
   ],
   "source": [
    "def mf_fold_error(ratings, train_indices, test_indices, users, movies, factors, n_training_steps, lr, lam):\n",
    "    \"\"\"\n",
    "    For a given fold, train the UM matrix and keep track of the rating errors on the train and test set.\n",
    "    \n",
    "    ratings: Dataframe, dataframe containing the ratings\n",
    "    train_indices: array, indices of the training set\n",
    "    test_indices array, indices of the test set\n",
    "    users: Dataframe, dataframe containing the users\n",
    "    movies: Dataframe, dataframe containing the movies\n",
    "    factors: int, the number of factors to use for the U and M matrix\n",
    "    n_training_steps: int, the number of steps for training\n",
    "    lr: float, learning rate \n",
    "    lam: float, lambda (used for regularization)\n",
    "    \"\"\"\n",
    "    n_users = len(users)\n",
    "    n_movies = len(movies)\n",
    "    \n",
    "    U = np.random.normal(0.0, 1.0, ((n_users, factors)))\n",
    "    M = np.random.normal(0.0, 1.0, ((factors, n_movies)))\n",
    "    \n",
    "    train_set = ratings.iloc[train_indices]\n",
    "    test_set = ratings.iloc[test_indices]\n",
    "\n",
    "    train_user_indices = train_set['UserID'].values - 1\n",
    "    test_user_indices = test_set['UserID'].values - 1\n",
    "\n",
    "    movie_ids = movies['MovieID'].values\n",
    "    train_movie_indices = make_movie_indices(train_set, movie_ids)\n",
    "    test_movie_indices = make_movie_indices(test_set, movie_ids)\n",
    "    \n",
    "    train_set_rating_matrix = np.zeros((n_users, n_movies))\n",
    "    for u, m, rating in zip(train_user_indices, train_movie_indices, train_set['Rating']):\n",
    "        train_set_rating_matrix[u, m] = rating\n",
    "\n",
    "    train_rmses = np.array([])\n",
    "    test_rmses = np.array([])\n",
    "\n",
    "    train_maes = np.array([])\n",
    "    test_maes = np.array([])\n",
    "\n",
    "    for step in range(n_training_steps):\n",
    "        predicted_ratings = np.dot(U, M)\n",
    "\n",
    "        train_set_predicted_ratings = predicted_ratings[train_user_indices, train_movie_indices]\n",
    "        test_set_predicted_ratings = predicted_ratings[test_user_indices, test_movie_indices]\n",
    "\n",
    "        train_errors = train_set['Rating'] - crop_ratings(train_set_predicted_ratings)\n",
    "        test_errors = test_set['Rating'] - crop_ratings(test_set_predicted_ratings)\n",
    "        \n",
    "        train_rmse, test_rmse = rmse(train_errors), rmse(test_errors)\n",
    "        \n",
    "        train_rmses = np.append(train_rmses, train_rmse)\n",
    "        test_rmses = np.append(test_rmses, test_rmse)\n",
    "\n",
    "        train_maes = np.append(train_maes, mae(train_errors))\n",
    "        test_maes = np.append(test_maes, mae(test_errors))\n",
    "\n",
    "        mp_lock.acquire()\n",
    "        print('step ', step, ' rmse train ', train_rmse, ' rmse test ', test_rmse)\n",
    "        mp_lock.release()\n",
    "        \n",
    "        for u, m in zip(train_user_indices, train_movie_indices):\n",
    "            u_vec, m_vec = U[u, :], M[:, m]\n",
    "            error = train_set_rating_matrix[u, m] - np.dot(u_vec, m_vec)\n",
    "            U[u, :] += lr * (2 * error * m_vec - lam * u_vec)\n",
    "            M[:, m] += lr * (2 * error * U[u, :] - lam * m_vec)\n",
    "    \n",
    "    return train_rmses, test_rmses, train_maes, test_maes\n",
    "\n",
    "\n",
    "def matrix_factorization(data, factors, n_training_steps, lr, lam, n_folds):\n",
    "    \"\"\"\n",
    "    Calculates the error for the matrix factorization model. Bias in training and test set \n",
    "    selection is reduced by using cross-validation. To speed up the process, we use multiprocessing \n",
    "    to divide the folds over different cores.\n",
    "    \n",
    "    data: tuple, contains the user, movie and rating dataframe\n",
    "    factors: int, the number of factors to use for the U and M matrix\n",
    "    n_training_steps: int, the number of steps for training\n",
    "    lr: float, learning rate \n",
    "    lam: float, lambda (used for regularization)\n",
    "    n_folds: int, the number of folds to use for cross-validation\n",
    "    \"\"\"\n",
    "    users, movies, ratings = data\n",
    "\n",
    "    cv = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n",
    "\n",
    "    print('Testing matrix factorization model...')\n",
    "\n",
    "    params = [(ratings,\n",
    "               train_indices, \n",
    "               test_indices, \n",
    "               users, \n",
    "               movies, \n",
    "               factors, \n",
    "               n_training_steps, \n",
    "               lr, \n",
    "               lam) for train_indices, test_indices in cv.split(ratings)]\n",
    "    pool = Pool(n_folds)\n",
    "    fold_errors = pool.starmap(mf_fold_error, params)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    train_rmses_per_fold = np.array([])\n",
    "    test_rmses_per_fold = np.array([])\n",
    "\n",
    "    train_maes_per_fold = np.array([])\n",
    "    test_maes_per_fold = np.array([])\n",
    "    \n",
    "    for errors in fold_errors:\n",
    "        train_rmses, test_rmses, train_maes, test_maes = errors\n",
    "        train_rmses_per_fold = np.append(train_rmses_per_fold, train_rmses)\n",
    "        test_rmses_per_fold = np.append(test_rmses_per_fold, test_rmses)\n",
    "        train_maes_per_fold = np.append(train_maes_per_fold, train_maes)\n",
    "        test_maes_per_fold = np.append(test_maes_per_fold, test_maes)\n",
    "    \n",
    "    return np.stack((train_rmses_per_fold, \n",
    "                     test_rmses_per_fold, \n",
    "                     train_maes_per_fold, \n",
    "                     test_maes_per_fold))\n",
    "\n",
    "number_of_folds = 5\n",
    "number_of_training_steps = 75\n",
    "number_of_factors = 10\n",
    "learning_rate = 0.005\n",
    "regularization = 0.05\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "errors = matrix_factorization(data,\n",
    "                              factors=number_of_factors,\n",
    "                              n_training_steps=number_of_training_steps,\n",
    "                              lr=learning_rate,\n",
    "                              lam=regularization,\n",
    "                              n_folds=number_of_folds)\n",
    "\n",
    "duration = time.time() - t0\n",
    "\n",
    "print('Run time: ', duration, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train rmse:  0.7804319447057322\n",
      "Final test rmse:  0.8859206597041049\n",
      "Final train mae:  0.6144913398872665\n",
      "Final test mae:  0.6922530922628514\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_train_rmses = np.mean(np.reshape(errors[0], (number_of_folds, number_of_training_steps)), axis=0)\n",
    "mean_test_rmses = np.mean(np.reshape(errors[1], (number_of_folds, number_of_training_steps)), axis=0)\n",
    "mean_train_maes = np.mean(np.reshape(errors[2], (number_of_folds, number_of_training_steps)), axis=0)\n",
    "mean_test_maes = np.mean(np.reshape(errors[3], (number_of_folds, number_of_training_steps)), axis=0)\n",
    "\n",
    "print('Final train rmse: ', mean_train_rmses[-1])\n",
    "print('Final test rmse: ', mean_test_rmses[-1])\n",
    "print('Final train mae: ', mean_train_maes[-1])\n",
    "print('Final test mae: ', mean_test_maes[-1])\n",
    "\n",
    "training_steps = np.arange(number_of_training_steps)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(training_steps, mean_train_rmses, label='train rmse', rasterized=True)\n",
    "ax.plot(training_steps, mean_test_rmses, label='test rmse', rasterized=True)\n",
    "ax.plot(training_steps, mean_train_maes, label='train mae', rasterized=True)\n",
    "ax.plot(training_steps, mean_test_maes, label='test mae', rasterized=True)\n",
    "ax.set_title('Matrix factorization errors')\n",
    "ax.set_xlabel('training step')\n",
    "ax.set_ylabel('error')\n",
    "ax.legend()\n",
    "plt.savefig('mf_errors.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
